# -*- coding: utf-8 -*-
"""Portfolios with Lower Volatility - Period 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yPEd03qzIzZjhrKH_caWAd_TdfFNz8Jy
"""

from google.colab import drive
drive.mount('/content/drive')
!ls "/content/drive/My Drive/thesis_files/"

import community
import networkx as nx
from community import community_louvain
import matplotlib.cm as cm
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cvxpy as cp

import pandas as pd
#clean and sort semiconductor companies
file_path =  "/content/drive/My Drive/thesis_files/Semiconductor Prices 2000 - 2023.csv"
data = pd.read_csv(file_path)
data = data.dropna(subset=['adjclose'])
data = data.sort_values(by=["InfoCode", "MarketDate"])
desired_companies = ['NVIDIA CORP.','TAIWAN SEMICON.MNFG.CTD.', 'RAMBUS INCO.', 'BROADCOM PTE LTD.', 'SAMSUNG SDI COMPANY LTD.','QUALCOMM INCO.','INTEL CORP.', 'TEXAS INSTRUMENTS INCO.', 'MICRON TECHNOLOGY INCO.', 'ARM HOLDINGS PLC.', 'ANALOG DEVICES INCO.', 'SK HYNIX INCO.', 'KLA CORP.', 'NXP SEMICONDUCTORS NV', 'MEDIATEK INCO.' , 'MICROCHIP TECH.INCO.','INFINEON TECHNOLOGIES AG', 'STMICROELECTRONICS NV', 'MONOLITHIC PWR.SYS.INCO.', 'ON SEMICONDUCTOR CORP.', 'LASERTEC CORP.','ENTEGRIS INCO.', 'SKYWORKS SOLUTIONS INCO.', 'BE SEMICONDUCTOR INDS.',  'QORVO INCO.', 'LATTICE SEMICON.CORP.', 'REALTEK SEMICON.CORP.',   'GLOBALWAFERS CTD.',  'COHERENT INCO.', 'AMKOR TECH.INCO.', 'ALCHIP TECHNOLOGIES LTD.', 'MACOM TGSN.HDG.INCO.',  'NANYA TECHNOLOGY CORP.', 'ROHM COMPANY LIMITED', 'EMEMORY TECHNOLOGY INCO.', 'GLOBAL UNICHIP CORP.' , 'CIRRUS LOGIC INCO.', 'SILICON LABS.INCO.', 'WOLFSPEED INCO.', 'AXCELIS TECHS.INCO.'  ]

filtered_data = data[data['DsCmpyName'].isin(desired_companies)]

start_date = '2016-01-01'
end_date = '2023-11-08'
official_data = filtered_data[(filtered_data['MarketDate'] >= start_date) & (filtered_data['MarketDate'] <= end_date)]

official_data['log_return'] = official_data.groupby('InfoCode')['adjclose'].transform(lambda x: np.log(x / x.shift(1)))
# Fill missing log returns with rolling mean of the last 2 values
official_data['log_return'] = official_data.groupby('InfoCode')['log_return'].transform(lambda x: x.fillna(x.rolling(2, min_periods=1).mean()))
official_data = official_data[official_data['InfoCode'] != 2231]
official_data

##pivot the semiconductor data and organize better
semi_sort_more = official_data.sort_values(by='MarketDate')
semi_sort_more = official_data.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data
pivot_data = semi_sort_more.pivot(index='MarketDate', columns='DsCmpyName', values='adjclose')
pivot_data.reset_index(inplace=True)
pivot_data['MarketDate'] = pd.to_datetime(pivot_data['MarketDate'])

def fill_with_previous_average(df):
    for column in df.select_dtypes(include=[np.number]):
        for i in range(2, len(df)):
            if np.isnan(df.at[df.index[i], column]):
                non_nan_values = df[column][:i].dropna().tail(2)
                if len(non_nan_values) == 2:
                    df.at[df.index[i], column] = non_nan_values.mean()
    return df

pivot_data_numeric = pivot_data.drop(columns=['MarketDate'])

pivot_data_numeric_filled = fill_with_previous_average(pivot_data_numeric)


pivot_data_filled = pd.concat([pivot_data[['MarketDate']], pivot_data_numeric_filled], axis=1)

pivot_data_filled

###PERIOD 2
##period 2 of semiconductor data (network) + calculate structural entropy
period2_start = '2018-12-31'
period2_end = '2022-11-30'
period2_data_semi = pivot_data_filled[(pivot_data_filled['MarketDate'] >= period2_start) & (pivot_data_filled['MarketDate'] <= period2_end)]

company_to_remove = 'ARM HOLDINGS PLC.'

if company_to_remove in period2_data_semi.columns:
    period2_data_semi.drop(columns=[company_to_remove], inplace=True)

period2_data_semi_numeric = period2_data_semi.drop(columns=['MarketDate']).reset_index(drop=True)
corr_matrix_period2_semi = period2_data_semi_numeric.corr(method='pearson')


G_semi_period2 = nx.Graph()

thresh2 = 0.7
for i in corr_matrix_period2_semi.columns:
  for j in corr_matrix_period2_semi.columns:
    if i != j and abs(corr_matrix_period2_semi[i][j]) > thresh2:
        G_semi_period2.add_edge(i, j)

#Apply Louvain community detection
partition2 = community_louvain.best_partition(G_semi_period2)
modularity2 = community_louvain.modularity(partition2, G_semi_period2)

values2 = [partition2.get(node) for node in G_semi_period2.nodes()]

node_to_company2 = {}

for node in G_semi_period2.nodes():
    if node in official_data['DsCmpyName'].values:
        company_name = official_data.loc[official_data['DsCmpyName'] == node, 'DsCmpyName'].iloc[0]
        node_to_company2[node] = company_name


plt.figure(figsize=(10, 10))
pos = nx.spring_layout(G_semi_period2)  # Compute node positions
nx.draw(G_semi_period2, pos, cmap=plt.get_cmap('jet'), node_color=values2, node_size=30, with_labels=False)
nx.draw_networkx_labels(G_semi_period2, pos, labels={node: name for node, name in node_to_company2.items() if node in G_semi_period2.nodes()}, font_size=8)  # Add labels
plt.show()


print("Total number of communities:", len(set(values2)))

# Calculate the sizes of each community
community_sizes2 = np.array([list(values2).count(i) for i in set(values2)])

# Compute the probability distribution of community sizes
prob_distribution2 = community_sizes2 / np.sum(community_sizes2)

# Calculate the entropy
structural_entropy2 = -np.sum(prob_distribution2 * np.log(prob_distribution2))

print("Structural Entropy:", structural_entropy2)

import numpy as np
import networkx as nx
import pandas as pd
import matplotlib.pyplot as plt
import cvxpy as cp


company_columns2 = [col for col in corr_matrix_period2_semi.columns if col != 'MarketDate']

corr_matrix_period2_semi_filtered = corr_matrix_period2_semi.loc[company_columns2, company_columns2]

# Transform correlations to distances
corr_matrix_array2 = corr_matrix_period2_semi_filtered.to_numpy()
corr_matrix_array2_transformed = 1 - corr_matrix_array2

G2_mst = nx.from_numpy_array(corr_matrix_array2_transformed)

mst2 = nx.minimum_spanning_tree(G2_mst)


strongest_edges2 = sorted(mst2.edges(data=True), key=lambda x: x[2]['weight'])[:2]
print("Edges with strongest correlations based on edge weights:", strongest_edges2)



node_degrees2 = dict(mst2.degree())
critical_nodes_degree2 = [node for node, degree in node_degrees2.items() if degree == max(node_degrees2.values())]
print("Critical nodes based on node degrees:", critical_nodes_degree2)


betweenness_centrality2 = nx.betweenness_centrality(mst2)
critical_nodes_bc2 = [node for node, bc in betweenness_centrality2.items() if bc == max(betweenness_centrality2.values())]
print("Critical nodes based on betweenness centrality:", critical_nodes_bc2)

node_to_company2 = {i: company for i, company in enumerate(corr_matrix_period2_semi_filtered.columns)}


plt.figure(figsize=(10, 8))
pos2 = nx.spring_layout(mst2)
nx.draw(mst2, pos2, labels=node_to_company2, with_labels=True, node_size=300, node_color='skyblue', edge_color='k', linewidths=0.5)
nx.draw_networkx_nodes(mst2, pos2, nodelist=critical_nodes_degree2, node_color='red', node_size=500)
nx.draw_networkx_nodes(mst2, pos2, nodelist=critical_nodes_bc2, node_color='green', node_size=500)
nx.draw_networkx_edges(mst2, pos2, edgelist=[(u, v) for u, v, _ in strongest_edges2], edge_color='blue', width=2)
plt.title('Minimum Spanning Tree for Semiconductor Companies - Period 2')
plt.show()

##FIND HIGHEST CENTRALITY - PERIOD 2
# Define weights for centrality measures
weights = {'eigenvector_centrality': 1/3, 'closeness_centrality': 1/3, 'betweenness_centrality': 1/3}

# Find representative stocks for each community based on centrality measures
community_representatives2_max = {}
for community_id in set(values2):
    community_nodes = [node for node, comm in partition2.items() if comm == community_id]
    representative_stock2_max = max(community_nodes, key=lambda x:
                               weights['eigenvector_centrality'] * nx.eigenvector_centrality(G_semi_period2)[x] +
                               weights['closeness_centrality'] * nx.closeness_centrality(G_semi_period2)[x] +
                               weights['betweenness_centrality'] * nx.betweenness_centrality(G_semi_period2)[x])
    community_representatives2_max[community_id] = representative_stock2_max

print("Representative stocks for each community for Period 2 (Max):")
for community_id, stock in community_representatives2_max.items():
    print(f"Community {community_id}: {stock}")


weights = {'eigenvector_centrality': 1/3, 'closeness_centrality': 1/3, 'betweenness_centrality': 1/3}

community_representatives2_min = {}
for community_id in set(values2):
    community_nodes = [node for node, comm in partition2.items() if comm == community_id]
    representative_stock2_min = min(community_nodes, key=lambda x:
                               weights['eigenvector_centrality'] * nx.eigenvector_centrality(G_semi_period2)[x] +
                               weights['closeness_centrality'] * nx.closeness_centrality(G_semi_period2)[x] +
                               weights['betweenness_centrality'] * nx.betweenness_centrality(G_semi_period2)[x])
    community_representatives2_min[community_id] = representative_stock2_min

print("Representative stocks for each community for Period 2 (Min):")
for community_id, stock in community_representatives2_min.items():
    print(f"Community {community_id}: {stock}")

#FIND HIGHEST AND 2ND HIGHEST CENTRALITY - PERIOD 2
weights = {'eigenvector_centrality': 1/3, 'closeness_centrality': 1/3, 'betweenness_centrality': 1/3}

# Find representative stocks for each community based on centrality measures
community_representatives2_max_extra = {}
community_representatives2_min_extra = {}

for community_id in set(values2):
    community_nodes = [node for node, comm in partition2.items() if comm == community_id]

    sorted_nodes = sorted(community_nodes, key=lambda x: (
        weights['eigenvector_centrality'] * nx.eigenvector_centrality(G_semi_period2)[x] +
        weights['closeness_centrality'] * nx.closeness_centrality(G_semi_period2)[x] +
        weights['betweenness_centrality'] * nx.betweenness_centrality(G_semi_period2)[x]
    ), reverse=True)  # Sort in descending order for highest centrality

    # Select top 2 stocks with highest centrality
    top_2_max = sorted_nodes[:2]
    community_representatives2_max_extra[community_id] = top_2_max

    sorted_nodes_reverse = sorted(community_nodes, key=lambda x: (
        weights['eigenvector_centrality'] * nx.eigenvector_centrality(G_semi_period2)[x] +
        weights['closeness_centrality'] * nx.closeness_centrality(G_semi_period2)[x] +
        weights['betweenness_centrality'] * nx.betweenness_centrality(G_semi_period2)[x]
    ))

    # Select top 2 stocks with lowest centrality
    top_2_min = sorted_nodes_reverse[:2]
    community_representatives2_min_extra[community_id] = top_2_min

print("Top 2 representative stocks for each community for Period 2 (Max):")
for community_id, stocks in community_representatives2_max_extra.items():
    print(f"Community {community_id}: {stocks}")

print("\nTop 2 representative stocks for each community for Period 2 (Min):")
for community_id, stocks in community_representatives2_min_extra.items():
    print(f"Community {community_id}: {stocks}")

import cvxpy as cp
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


start_date2 = '2018-12-31'
end_date2 = '2022-11-30'
official_data_period2 = official_data[(official_data['MarketDate'] >= start_date2) &
                                      (official_data['MarketDate'] <= end_date2)]

max_centrality_stocks2 = list(community_representatives2_max.values())

portfolio1_2_stocks = official_data_period2[official_data_period2['DsCmpyName'].isin(max_centrality_stocks2)]

# Calculate log returns
portfolio1_2_stocks['log_return'] = np.log(portfolio1_2_stocks['adjclose'] / portfolio1_2_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio1_2_stocks = portfolio1_2_stocks.dropna(subset=['log_return'])
portfolio1_2_stocks = portfolio1_2_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns1_2 = portfolio1_2_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns1_2 = log_returns1_2.mean().values
expected_returns1_2 = expected_returns1_2.reshape(-1, 1)
cov_matrix1_2 = log_returns1_2.cov()

n_assets1_2 = expected_returns1_2.shape[0]

company_names1_2 = log_returns1_2.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights1_2 = cp.Variable(n_assets1_2)
portfolio_return1_2 = expected_returns1_2.T @ weights1_2
portfolio_risk1_2 = cp.quad_form(weights1_2, cov_matrix1_2)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns1_2 = []
efficient_frontier_risks1_2 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem1_2 = cp.Problem(cp.Maximize(portfolio_return1_2 - gamma * portfolio_risk1_2),
                         [cp.sum(weights1_2) == 1, weights1_2 >= 0])
    problem1_2.solve()
    annualized_return1_2 = np.exp(portfolio_return1_2.value * 252) - 1
    annualized_volatility1_2 = np.sqrt(portfolio_risk1_2.value) * np.sqrt(252)
    efficient_frontier_returns1_2.append(annualized_return1_2.item())
    efficient_frontier_risks1_2.append(annualized_volatility1_2.item())

annualized_efficient_frontier_risks = np.array(efficient_frontier_risks1_2)

index_closest_volatility = np.argmin(np.abs(annualized_efficient_frontier_risks - target_volatility))

expected_return_at_target_volatility = efficient_frontier_returns1_2[index_closest_volatility]

# Output the expected annualized return
print(f"Expected Annualized Return at {target_volatility*100}% Volatility: {expected_return_at_target_volatility:.2%}")

# Plot the efficient frontier with annualized values
plt.figure(figsize=(10, 6))
plt.plot(annualized_efficient_frontier_risks, efficient_frontier_returns1_2, 'b-', label='Efficient Frontier')
plt.scatter(annualized_efficient_frontier_risks[index_closest_volatility], expected_return_at_target_volatility, color='red', label=f'Closest Point to {target_volatility*100}% Volatility')
plt.title(f'Efficient Frontier and Closest Point to {target_volatility*100}% Annualized Volatility')
plt.xlabel('Annualized Volatility (Standard Deviation)')
plt.ylabel('Expected Annualized Return')
plt.legend()
plt.grid(True)
plt.show()

import cvxpy as cp
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import norm


start_date2 = '2018-12-31'
end_date2 = '2022-11-30'
official_data_period2 = official_data[(official_data['MarketDate'] >= start_date2) &
                                      (official_data['MarketDate'] <= end_date2)]

# Identify the core node (most representative stock)
core_node2_2 = community_representatives2_max[0]


# Find the 1st cousins of the core node
first_cousins2_2 = [node for node in G_semi_period2.neighbors(core_node2_2)]

filtered_cousins2_2 = [node for node in first_cousins2_2 if abs(corr_matrix_period2_semi[core_node2_2][node]) > 0.7]

portfolio2_2_stocks = official_data_period2[official_data_period2['DsCmpyName'].isin(filtered_cousins2_2)]

# Calculate log returns
portfolio2_2_stocks['log_return'] = np.log(portfolio2_2_stocks['adjclose'] / portfolio2_2_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio2_2_stocks = portfolio2_2_stocks.dropna(subset=['log_return'])
portfolio2_2_stocks = portfolio2_2_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns2_2 = portfolio2_2_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns2_2 = log_returns2_2.mean().values
expected_returns2_2 = expected_returns2_2.reshape(-1, 1)
cov_matrix2_2 = log_returns2_2.cov()

n_assets2_2 = expected_returns2_2.shape[0]

company_names2_2 = log_returns2_2.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights2_2 = cp.Variable(n_assets2_2)
portfolio_return2_2 = expected_returns2_2.T @ weights2_2
portfolio_risk2_2 = cp.quad_form(weights2_2, cov_matrix2_2)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns2_2 = []
efficient_frontier_risks2_2 = []
sharpe_ratios2_2 = []
gamma_values_with_sharpe2_2 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem2_2 = cp.Problem(cp.Maximize(portfolio_return2_2 - gamma * portfolio_risk2_2),
                         [cp.sum(weights2_2) == 1, weights2_2 >= 0])
    problem2_2.solve()
    annualized_return2_2 = np.exp(portfolio_return2_2.value * 252) - 1
    annualized_volatility2_2 = np.sqrt(portfolio_risk2_2.value) * np.sqrt(252)
    efficient_frontier_returns2_2.append(annualized_return2_2.item())
    efficient_frontier_risks2_2.append(annualized_volatility2_2.item())

    sharpe_ratio2_2 = (annualized_return2_2 - risk_free_rate) / annualized_volatility2_2
    sharpe_ratios2_2.append(sharpe_ratio2_2)
    gamma_values_with_sharpe2_2.append(gamma.value)

# Target volatility
target_volatility = 0.
efficient_frontier_risks2_2 = np.array(efficient_frontier_risks2_2)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks2_2 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns2_2[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights2_2.value):
    if weight > 0:
        print(f"{company_names2_2[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks2_2, efficient_frontier_returns2_2, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks2_2[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

import cvxpy as cp
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import norm


start_date2 = '2018-12-31'
end_date2 = '2022-11-30'
official_data_period2 = official_data[(official_data['MarketDate'] >= start_date2) &
                                      (official_data['MarketDate'] <= end_date2)]

# Identify the core node
core_node3_2 = community_representatives2_max[1]


# Find the 1st cousins of the core node
first_cousins3_2 = [node for node in G_semi_period2.neighbors(core_node3_2)]

filtered_cousins3_2 = [node for node in first_cousins3_2 if abs(corr_matrix_period2_semi[core_node3_2][node]) > 0.7]

portfolio3_2_stocks = official_data_period2[official_data_period2['DsCmpyName'].isin(filtered_cousins3_2)]

# Calculate log returns
portfolio3_2_stocks['log_return'] = np.log(portfolio3_2_stocks['adjclose'] / portfolio3_2_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio3_2_stocks = portfolio3_2_stocks.dropna(subset=['log_return'])
portfolio3_2_stocks = portfolio3_2_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns3_2 = portfolio3_2_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns3_2 = log_returns3_2.mean().values
expected_returns3_2 = expected_returns3_2.reshape(-1, 1)
cov_matrix3_2 = log_returns3_2.cov()

n_assets3_2 = expected_returns3_2.shape[0]

company_names3_2 = log_returns3_2.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights3_2 = cp.Variable(n_assets3_2)
portfolio_return3_2 = expected_returns3_2.T @ weights3_2
portfolio_risk3_2 = cp.quad_form(weights3_2, cov_matrix3_2)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns3_2 = []
efficient_frontier_risks3_2 = []
sharpe_ratios3_2 = []
gamma_values_with_sharpe3_2 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem3_2 = cp.Problem(cp.Maximize(portfolio_return3_2 - gamma * portfolio_risk3_2),
                         [cp.sum(weights3_2) == 1, weights3_2 >= 0])
    problem3_2.solve()
    annualized_return3_2 = np.exp(portfolio_return3_2.value * 252) - 1
    annualized_volatility3_2 = np.sqrt(portfolio_risk3_2.value) * np.sqrt(252)
    efficient_frontier_returns3_2.append(annualized_return3_2.item())
    efficient_frontier_risks3_2.append(annualized_volatility3_2.item())


    sharpe_ratio3_2 = (annualized_return3_2 - risk_free_rate) / annualized_volatility3_2
    sharpe_ratios3_2.append(sharpe_ratio3_2)
    gamma_values_with_sharpe3_2.append(gamma.value)

# Target volatility
target_volatility = 0.2
efficient_frontier_risks3_2 = np.array(efficient_frontier_risks3_2)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks3_2 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns3_2[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights3_2.value):
    if weight > 0:
        print(f"{company_names3_2[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks3_2, efficient_frontier_returns3_2, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks3_2[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

node_to_community2_0 = partition2
portfolio_stocks2_0 = list(G_semi_period2.nodes)
portfolio_stocks_community2_0 = [stock for stock in portfolio_stocks2_0 if stock in G_semi_period2.nodes() and node_to_community2_0.get(stock, -1) == 0]

start_date = '2018-12-31'
end_date = '2022-11-30'
official_data_period2 = official_data[(official_data['MarketDate'] >= start_date2) &
                                      (official_data['MarketDate'] <= end_date2)]

portfolio4_2_stocks = official_data_period2[official_data_period2['DsCmpyName'].isin(portfolio_stocks_community2_0)]

# Calculate log returns
portfolio4_2_stocks['log_return'] = np.log(portfolio4_2_stocks['adjclose'] / portfolio4_2_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio4_2_stocks = portfolio4_2_stocks.dropna(subset=['log_return'])
portfolio4_2_stocks = portfolio4_2_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns4_2 = portfolio4_2_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns4_2 = log_returns4_2.mean().values
expected_returns4_2 = expected_returns4_2.reshape(-1, 1)
cov_matrix4_2 = log_returns4_2.cov()

n_assets4_2 = expected_returns4_2.shape[0]

company_names4_2 = log_returns4_2.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights4_2 = cp.Variable(n_assets4_2)
portfolio_return4_2 = expected_returns4_2.T @ weights4_2
portfolio_risk4_2 = cp.quad_form(weights4_2, cov_matrix4_2)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns4_2 = []
efficient_frontier_risks4_2 = []
sharpe_ratios4_2 = []
gamma_values_with_sharpe4_2 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem4_2 = cp.Problem(cp.Maximize(portfolio_return4_2 - gamma * portfolio_risk4_2),
                         [cp.sum(weights4_2) == 1, weights4_2 >= 0])
    problem4_2.solve()
    annualized_return4_2 = np.exp(portfolio_return4_2.value * 252) - 1
    annualized_volatility4_2 = np.sqrt(portfolio_risk4_2.value) * np.sqrt(252)
    efficient_frontier_returns4_2.append(annualized_return4_2.item())
    efficient_frontier_risks4_2.append(annualized_volatility4_2.item())

    sharpe_ratio4_2 = (annualized_return4_2 - risk_free_rate) / annualized_volatility4_2
    sharpe_ratios4_2.append(sharpe_ratio4_2)
    gamma_values_with_sharpe4_2.append(gamma.value)

# Target volatility
target_volatility = 0.20
efficient_frontier_risks4_2 = np.array(efficient_frontier_risks4_2)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks4_2 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns4_2[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights4_2.value):
    if weight > 0:
        print(f"{company_names4_2[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks4_2, efficient_frontier_returns4_2, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks4_2[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

node_to_community2_1 = partition2
portfolio_stocks2_1 = list(G_semi_period2.nodes)
portfolio_stocks_community2_1 = [stock for stock in portfolio_stocks2_1 if stock in G_semi_period2.nodes() and node_to_community2_1.get(stock, -1) == 1]

start_date = '2018-12-31'
end_date = '2022-11-30'
official_data_period2 = official_data[(official_data['MarketDate'] >= start_date2) &
                                      (official_data['MarketDate'] <= end_date2)]

portfolio5_2_stocks = official_data_period2[official_data_period2['DsCmpyName'].isin(portfolio_stocks_community2_1)]

# Calculate log returns
portfolio5_2_stocks['log_return'] = np.log(portfolio5_2_stocks['adjclose'] / portfolio5_2_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio5_2_stocks = portfolio5_2_stocks.dropna(subset=['log_return'])
portfolio5_2_stocks = portfolio5_2_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns5_2 = portfolio5_2_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns5_2 = log_returns5_2.mean().values
expected_returns5_2 = expected_returns5_2.reshape(-1, 1)
cov_matrix5_2 = log_returns5_2.cov()

n_assets5_2 = expected_returns5_2.shape[0]

company_names5_2 = log_returns5_2.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights5_2 = cp.Variable(n_assets5_2)
portfolio_return5_2= expected_returns5_2.T @ weights5_2
portfolio_risk5_2 = cp.quad_form(weights5_2, cov_matrix5_2)

# Risk-free rate
risk_free_rate = 0.02


efficient_frontier_returns5_2 = []
efficient_frontier_risks5_2 = []
sharpe_ratios5_2 = []
gamma_values_with_sharpe5_2 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem5_2 = cp.Problem(cp.Maximize(portfolio_return5_2 - gamma * portfolio_risk5_2),
                         [cp.sum(weights5_2) == 1, weights5_2 >= 0])
    problem5_2.solve()
    annualized_return5_2 = np.exp(portfolio_return5_2.value * 252) - 1
    annualized_volatility5_2 = np.sqrt(portfolio_risk5_2.value) * np.sqrt(252)
    efficient_frontier_returns5_2.append(annualized_return5_2.item())
    efficient_frontier_risks5_2.append(annualized_volatility5_2.item())

    sharpe_ratio5_2 = (annualized_return5_2 - risk_free_rate) / annualized_volatility5_2
    sharpe_ratios5_2.append(sharpe_ratio5_2)
    gamma_values_with_sharpe5_2.append(gamma.value)

# Target volatility
target_volatility = 0.20
efficient_frontier_risks5 = np.array(efficient_frontier_risks5_2)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks5 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns5_2[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights5_2.value):
    if weight > 0:
        print(f"{company_names5_2[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks5, efficient_frontier_returns5_2, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks5_2[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

representative_stocks2_1 = []
for community_id, stock in community_representatives2_max_extra.items():
    representative_stocks2_1.append(stock)

flat_representative_stocks2_1 = [stock for tuple in representative_stocks2_1 for stock in tuple]

start_date = '2018-12-31'
end_date = '2022-11-30'
official_data_period2 = official_data[(official_data['MarketDate'] >= start_date2) &
                                      (official_data['MarketDate'] <= end_date2)]

portfolio6_2_stocks = official_data_period2[official_data_period2['DsCmpyName'].isin(flat_representative_stocks2_1)]

# Calculate log returns
portfolio6_2_stocks['log_return'] = np.log(portfolio6_2_stocks['adjclose'] / portfolio6_2_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio6_2_stocks = portfolio6_2_stocks.dropna(subset=['log_return'])
portfolio6_2_stocks = portfolio6_2_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns6_2 = portfolio6_2_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns6_2 = log_returns6_2.mean().values
expected_returns6_2 = expected_returns6_2.reshape(-1, 1)
cov_matrix6_2 = log_returns6_2.cov()

n_assets6_2 = expected_returns6_2.shape[0]

company_names6_2 = log_returns6_2.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights6_2 = cp.Variable(n_assets6_2)
portfolio_return6_2 = expected_returns6_2.T @ weights6_2
portfolio_risk6_2 = cp.quad_form(weights6_2, cov_matrix6_2)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns6_2 = []
efficient_frontier_risks6_2 = []
sharpe_ratios6_2 = []
gamma_values_with_sharpe6_2 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem6_2 = cp.Problem(cp.Maximize(portfolio_return6_2 - gamma * portfolio_risk6_2),
                         [cp.sum(weights6_2) == 1, weights6_2 >= 0])
    problem6_2.solve()
    annualized_return6_2 = np.exp(portfolio_return6_2.value * 252) - 1
    annualized_volatility6_2 = np.sqrt(portfolio_risk6_2.value) * np.sqrt(252)
    efficient_frontier_returns6_2.append(annualized_return6_2.item())
    efficient_frontier_risks6_2.append(annualized_volatility6_2.item())

    sharpe_ratio6_2 = (annualized_return6_2 - risk_free_rate) / annualized_volatility6_2
    sharpe_ratios6_2.append(sharpe_ratio6_2)
    gamma_values_with_sharpe6_2.append(gamma.value)

# Target volatility
target_volatility = 0.20
efficient_frontier_risks6_2 = np.array(efficient_frontier_risks6_2)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks6_2 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns6_2[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights6_2.value):
    if weight > 0:
        print(f"{company_names6_2[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks6_2, efficient_frontier_returns6_2, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks6_2[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

representative_stocks2_2 = []
for community_id, stock in community_representatives2_max.items():
    representative_stocks2_2.append(stock)
    same_community_stocks2_2 = [node for node, comm in partition2.items() if comm == community_id]
    same_community_stocks2_2.remove(stock)
    if same_community_stocks2_2:
        representative_stocks2_2.append(same_community_stocks2_2[0])

start_date2 = '2018-12-31'
end_date2 = '2022-11-30'
official_data_period2 = official_data[(official_data['MarketDate'] >= start_date2) &
                                      (official_data['MarketDate'] <= end_date2)]

portfolio7_2_stocks = official_data_period2[official_data_period2['DsCmpyName'].isin(representative_stocks2_2)]

# Calculate log returns
portfolio7_2_stocks['log_return'] = np.log(portfolio7_2_stocks['adjclose'] / portfolio7_2_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio7_2_stocks = portfolio7_2_stocks.dropna(subset=['log_return'])
portfolio7_2_stocks = portfolio7_2_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns7_2 = portfolio7_2_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns7_2 = log_returns7_2.mean().values
expected_returns7_2 = expected_returns7_2.reshape(-1, 1)
cov_matrix7_2 = log_returns7_2.cov()

n_assets7_2 = expected_returns7_2.shape[0]

company_names7_2 = log_returns7_2.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights7_2 = cp.Variable(n_assets7_2)
portfolio_return7_2 = expected_returns7_2.T @ weights7_2
portfolio_risk7_2 = cp.quad_form(weights7_2, cov_matrix7_2)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns7_2 = []
efficient_frontier_risks7_2 = []
sharpe_ratios7_2 = []
gamma_values_with_sharpe7_2 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem7_2 = cp.Problem(cp.Maximize(portfolio_return7_2 - gamma * portfolio_risk7_2),
                         [cp.sum(weights7_2) == 1, weights7_2 >= 0])
    problem7_2.solve()
    annualized_return7_2 = np.exp(portfolio_return7_2.value * 252) - 1
    annualized_volatility7_2 = np.sqrt(portfolio_risk7_2.value) * np.sqrt(252)
    efficient_frontier_returns7_2.append(annualized_return7_2.item())
    efficient_frontier_risks7_2.append(annualized_volatility7_2.item())

    sharpe_ratio7_2 = (annualized_return7_2 - risk_free_rate) / annualized_volatility7_2
    sharpe_ratios7_2.append(sharpe_ratio7_2)
    gamma_values_with_sharpe7_2.append(gamma.value)

# Target volatility
target_volatility = 0.20
efficient_frontier_risks7_2 = np.array(efficient_frontier_risks7_2)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks7_2 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns7_2[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights7_2.value):
    if weight > 0:
        print(f"{company_names7_2[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks7_2, efficient_frontier_returns7_2, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks7_2[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

critical_nodes_all2 = set(critical_nodes_bc2 + critical_nodes_degree2)
for u, v, _ in strongest_edges2:
    critical_nodes_all2.add(u)
    critical_nodes_all2.add(v)

representative_stocks_mst2 = [node_to_company2[node] for node in critical_nodes_all2]

start_date2 = '2018-12-31'
end_date2 = '2022-11-30'
official_data_period2 = official_data[(official_data['MarketDate'] >= start_date2) &
                                      (official_data['MarketDate'] <= end_date2)]

portfolio8_2_stocks = official_data_period2[official_data_period2['DsCmpyName'].isin(representative_stocks_mst2)]

# Calculate log returns
portfolio8_2_stocks['log_return'] = np.log(portfolio8_2_stocks['adjclose'] / portfolio8_2_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio8_2_stocks = portfolio8_2_stocks.dropna(subset=['log_return'])
portfolio8_2_stocks = portfolio8_2_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns8_2 = portfolio8_2_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns8_2 = log_returns8_2.mean().values
expected_returns8_2 = expected_returns8_2.reshape(-1, 1)
cov_matrix8_2 = log_returns8_2.cov()

n_assets8_2 = expected_returns8_2.shape[0]

company_names8_2 = log_returns8_2.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights8_2 = cp.Variable(n_assets8_2)
portfolio_return8_2 = expected_returns8_2.T @ weights8_2
portfolio_risk8_2 = cp.quad_form(weights8_2, cov_matrix8_2)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns8_2 = []
efficient_frontier_risks8_2 = []
sharpe_ratios8_2 = []
gamma_values_with_sharpe8_2 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem8_2 = cp.Problem(cp.Maximize(portfolio_return8_2 - gamma * portfolio_risk8_2),
                         [cp.sum(weights8_2) == 1, weights8_2 >= 0])
    problem8_2.solve()
    annualized_return8_2 = np.exp(portfolio_return8_2.value * 252) - 1
    annualized_volatility8_2 = np.sqrt(portfolio_risk8_2.value) * np.sqrt(252)
    efficient_frontier_returns8_2.append(annualized_return8_2.item())
    efficient_frontier_risks8_2.append(annualized_volatility8_2.item())

    sharpe_ratio8_2 = (annualized_return8_2 - risk_free_rate) / annualized_volatility8_2
    sharpe_ratios8_2.append(sharpe_ratio8_2)
    gamma_values_with_sharpe8_2.append(gamma.value)

# Target volatility
target_volatility = 0.20
efficient_frontier_risks8_2 = np.array(efficient_frontier_risks8_2)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks8_2 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns8_2[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights8_2.value):
    if weight > 0:
        print(f"{company_names8_2[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks8_2, efficient_frontier_returns8_2, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks8_2[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()