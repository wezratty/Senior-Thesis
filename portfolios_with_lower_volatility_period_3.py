# -*- coding: utf-8 -*-
"""Portfolios with Lower Volatility - Period 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CJ9Aq397EN3cpB4eA7Ucu-fkc28rmvTo
"""

from google.colab import drive
drive.mount('/content/drive')
!ls "/content/drive/My Drive/thesis_files/"

import community
import networkx as nx
from community import community_louvain
import matplotlib.cm as cm
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cvxpy as cp

import pandas as pd
#clean and sort semiconductor companies
file_path =  "/content/drive/My Drive/thesis_files/Semiconductor Prices 2000 - 2023.csv"
data = pd.read_csv(file_path)
data = data.dropna(subset=['adjclose'])
data = data.sort_values(by=["InfoCode", "MarketDate"])
desired_companies = ['NVIDIA CORP.','TAIWAN SEMICON.MNFG.CTD.', 'RAMBUS INCO.', 'BROADCOM PTE LTD.', 'SAMSUNG SDI COMPANY LTD.','QUALCOMM INCO.','INTEL CORP.', 'TEXAS INSTRUMENTS INCO.', 'MICRON TECHNOLOGY INCO.', 'ARM HOLDINGS PLC.', 'ANALOG DEVICES INCO.', 'SK HYNIX INCO.', 'KLA CORP.', 'NXP SEMICONDUCTORS NV', 'MEDIATEK INCO.' , 'MICROCHIP TECH.INCO.','INFINEON TECHNOLOGIES AG', 'STMICROELECTRONICS NV', 'MONOLITHIC PWR.SYS.INCO.', 'ON SEMICONDUCTOR CORP.', 'LASERTEC CORP.','ENTEGRIS INCO.', 'SKYWORKS SOLUTIONS INCO.', 'BE SEMICONDUCTOR INDS.',  'QORVO INCO.', 'LATTICE SEMICON.CORP.', 'REALTEK SEMICON.CORP.',   'GLOBALWAFERS CTD.',  'COHERENT INCO.', 'AMKOR TECH.INCO.', 'ALCHIP TECHNOLOGIES LTD.', 'MACOM TGSN.HDG.INCO.',  'NANYA TECHNOLOGY CORP.', 'ROHM COMPANY LIMITED', 'EMEMORY TECHNOLOGY INCO.', 'GLOBAL UNICHIP CORP.' , 'CIRRUS LOGIC INCO.', 'SILICON LABS.INCO.', 'WOLFSPEED INCO.', 'AXCELIS TECHS.INCO.'  ]

filtered_data = data[data['DsCmpyName'].isin(desired_companies)]

start_date = '2016-01-01'
end_date = '2023-11-08'
official_data = filtered_data[(filtered_data['MarketDate'] >= start_date) & (filtered_data['MarketDate'] <= end_date)]

official_data['log_return'] = official_data.groupby('InfoCode')['adjclose'].transform(lambda x: np.log(x / x.shift(1)))
official_data['log_return'] = official_data.groupby('InfoCode')['log_return'].transform(lambda x: x.fillna(x.rolling(2, min_periods=1).mean()))
official_data = official_data[official_data['InfoCode'] != 2231]
official_data

##pivot the semiconductor data and organize better
semi_sort_more = official_data.sort_values(by='MarketDate')
semi_sort_more = official_data.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data
pivot_data = semi_sort_more.pivot(index='MarketDate', columns='DsCmpyName', values='adjclose')
pivot_data.reset_index(inplace=True)
pivot_data['MarketDate'] = pd.to_datetime(pivot_data['MarketDate'])


def fill_with_previous_average(df):
    for column in df.select_dtypes(include=[np.number]):
        for i in range(2, len(df)):
            if np.isnan(df.at[df.index[i], column]):
                non_nan_values = df[column][:i].dropna().tail(2)
                if len(non_nan_values) == 2:
                    df.at[df.index[i], column] = non_nan_values.mean()
    return df


pivot_data_numeric = pivot_data.drop(columns=['MarketDate'])

pivot_data_numeric_filled = fill_with_previous_average(pivot_data_numeric)

pivot_data_filled = pd.concat([pivot_data[['MarketDate']], pivot_data_numeric_filled], axis=1)

pivot_data_filled

##period 3 of semiconductor data (network) + calculate structural entropy
period3_start = '2022-11-30'
period3_end = '2023-11-08'
period3_data_semi = pivot_data_filled[(pivot_data_filled['MarketDate'] >= period3_start) & (pivot_data_filled['MarketDate'] <= period3_end)]
period3_data_semi.reset_index(drop=True, inplace=True)
corr_matrix_period3_semi = period3_data_semi.corr(method='pearson')

company_to_remove = 'COHERENT INCO.'

if company_to_remove in period3_data_semi.columns:
    period3_data_semi.drop(columns=[company_to_remove], inplace=True)

period3_data_semi_numeric = period3_data_semi.drop(columns=['MarketDate']).reset_index(drop=True)
corr_matrix_period3_semi = period3_data_semi_numeric.corr(method='pearson')

G_semi_period3 = nx.Graph()

thresh3 = 0.7
for i in corr_matrix_period3_semi.columns:
  for j in corr_matrix_period3_semi.columns:
    if i != j and abs(corr_matrix_period3_semi[i][j]) > thresh3:
        G_semi_period3.add_edge(i, j)

#Apply Louvain community detection
partition3 = community_louvain.best_partition(G_semi_period3)
modularity3 = community_louvain.modularity(partition3, G_semi_period3)

values3 = [partition3.get(node) for node in G_semi_period3.nodes()]

node_to_company3 = {}

for node in G_semi_period3.nodes():
    if node in official_data['DsCmpyName'].values:
        company_name = official_data.loc[official_data['DsCmpyName'] == node, 'DsCmpyName'].iloc[0]
        node_to_company3[node] = company_name

plt.figure(figsize=(10, 10))
pos = nx.spring_layout(G_semi_period3)
nx.draw(G_semi_period3, pos, cmap=plt.get_cmap('jet'), node_color=values3, node_size=30, with_labels=False)
nx.draw_networkx_labels(G_semi_period3, pos, labels={node: name for node, name in node_to_company3.items() if node in G_semi_period3.nodes()}, font_size=8)  # Add labels
plt.show()

print("Total number of communities:", len(set(values3)))


# Calculate the sizes of each community
community_sizes3 = np.array([list(values3).count(i) for i in set(values3)])

# Compute the probability distribution of community sizes
prob_distribution3 = community_sizes3 / np.sum(community_sizes3)

# Calculate the entropy
structural_entropy3 = -np.sum(prob_distribution3 * np.log(prob_distribution3))

print("Structural Entropy:", structural_entropy3)

import numpy as np
import networkx as nx
import pandas as pd
import matplotlib.pyplot as plt
import cvxpy as cp


company_columns3 = [col for col in corr_matrix_period3_semi.columns if col != 'MarketDate']

corr_matrix_period3_semi_filtered = corr_matrix_period3_semi.loc[company_columns3, company_columns3]

# Transform correlations to distances
corr_matrix_array3 = corr_matrix_period3_semi_filtered.to_numpy()
corr_matrix_array3_transformed = 1 - corr_matrix_array3

G3_mst = nx.from_numpy_array(corr_matrix_array3_transformed)

mst3 = nx.minimum_spanning_tree(G3_mst)

strongest_edges3 = sorted(mst3.edges(data=True), key=lambda x: x[2]['weight'])[:2]
print("Edges with strongest correlations based on edge weights:", strongest_edges3)

node_degrees3 = dict(mst3.degree())
critical_nodes_degree3 = [node for node, degree in node_degrees3.items() if degree == max(node_degrees3.values())]
print("Critical nodes based on node degrees:", critical_nodes_degree3)


betweenness_centrality3 = nx.betweenness_centrality(mst3)
critical_nodes_bc3 = [node for node, bc in betweenness_centrality3.items() if bc == max(betweenness_centrality3.values())]
print("Critical nodes based on betweenness centrality:", critical_nodes_bc3)

node_to_company3 = {i: company for i, company in enumerate(corr_matrix_period3_semi_filtered.columns)}

plt.figure(figsize=(10, 8))
pos3 = nx.spring_layout(mst3)
nx.draw(mst3, pos3, labels=node_to_company3, with_labels=True, node_size=300, node_color='skyblue', edge_color='k', linewidths=0.5)
nx.draw_networkx_nodes(mst3, pos3, nodelist=critical_nodes_degree3, node_color='red', node_size=500)
nx.draw_networkx_nodes(mst3, pos3, nodelist=critical_nodes_bc3, node_color='green', node_size=500)
nx.draw_networkx_edges(mst3, pos3, edgelist=[(u, v) for u, v, _ in strongest_edges3], edge_color='blue', width=2)
plt.title('Minimum Spanning Tree for Semiconductor Companies - Period 3')
plt.show()

##find representative stocks for period 3
# Define weights for centrality measures
weights = {'eigenvector_centrality': 1/3, 'closeness_centrality': 1/3, 'betweenness_centrality': 1/3}

# Find representative stocks for each community based on centrality measures
community_representatives3_max = {}
for community_id in set(values3):
    community_nodes = [node for node, comm in partition3.items() if comm == community_id]
    representative_stock3_max = max(community_nodes, key=lambda x:
                               weights['eigenvector_centrality'] * nx.eigenvector_centrality(G_semi_period3)[x] +
                               weights['closeness_centrality'] * nx.closeness_centrality(G_semi_period3)[x] +
                               weights['betweenness_centrality'] * nx.betweenness_centrality(G_semi_period3)[x])
    community_representatives3_max[community_id] = representative_stock3_max

print("Representative stocks for each community for Period 3 (Max):")
for community_id, stock in community_representatives3_max.items():
    print(f"Community {community_id}: {stock}")


weights = {'eigenvector_centrality': 1/3, 'closeness_centrality': 1/3, 'betweenness_centrality': 1/3}

community_representatives3_min = {}
for community_id in set(values3):
    community_nodes = [node for node, comm in partition3.items() if comm == community_id]
    representative_stock3_min = min(community_nodes, key=lambda x:
                               weights['eigenvector_centrality'] * nx.eigenvector_centrality(G_semi_period3)[x] +
                               weights['closeness_centrality'] * nx.closeness_centrality(G_semi_period3)[x] +
                               weights['betweenness_centrality'] * nx.betweenness_centrality(G_semi_period3)[x])
    community_representatives3_min[community_id] = representative_stock3_min

print("Representative stocks for each community for Period 3 (Min):")
for community_id, stock in community_representatives3_min.items():
    print(f"Community {community_id}: {stock}")

#FIND HIGHEST AND 2ND HIGHEST CENTRALITY - PERIOD 3
weights = {'eigenvector_centrality': 1/3, 'closeness_centrality': 1/3, 'betweenness_centrality': 1/3}

# Find representative stocks for each community based on centrality measures
community_representatives3_max_extra = {}
community_representatives3_min_extra = {}

for community_id in set(values3):
    community_nodes = [node for node, comm in partition3.items() if comm == community_id]

    sorted_nodes = sorted(community_nodes, key=lambda x: (
        weights['eigenvector_centrality'] * nx.eigenvector_centrality(G_semi_period3)[x] +
        weights['closeness_centrality'] * nx.closeness_centrality(G_semi_period3)[x] +
        weights['betweenness_centrality'] * nx.betweenness_centrality(G_semi_period3)[x]
    ), reverse=True)

    # Select top 2 stocks with highest centrality
    top_2_max = sorted_nodes[:2]
    community_representatives3_max_extra[community_id] = top_2_max

    sorted_nodes_reverse = sorted(community_nodes, key=lambda x: (
        weights['eigenvector_centrality'] * nx.eigenvector_centrality(G_semi_period3)[x] +
        weights['closeness_centrality'] * nx.closeness_centrality(G_semi_period3)[x] +
        weights['betweenness_centrality'] * nx.betweenness_centrality(G_semi_period3)[x]
    ))

    # Select top 2 stocks with lowest centrality
    top_2_min = sorted_nodes_reverse[:2]
    community_representatives3_min_extra[community_id] = top_2_min

print("Top 2 representative stocks for each community for Period 3 (Max):")
for community_id, stocks in community_representatives3_max_extra.items():
    print(f"Community {community_id}: {stocks}")

print("\nTop 2 representative stocks for each community for Period 3 (Min):")
for community_id, stocks in community_representatives3_min_extra.items():
    print(f"Community {community_id}: {stocks}")

import cvxpy as cp
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import norm


start_date3 = '2022-11-30'
end_date3 = '2023-11-08'
official_data_period3 = official_data[(official_data['MarketDate'] >= start_date3) &
                                      (official_data['MarketDate'] <= end_date3)]

max_centrality_stocks3 = list(community_representatives3_max.values())

portfolio1_3_stocks = official_data_period3[official_data_period3['DsCmpyName'].isin(max_centrality_stocks3)]

# Calculate log returns
portfolio1_3_stocks['log_return'] = np.log(portfolio1_3_stocks['adjclose'] / portfolio1_3_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio1_3_stocks = portfolio1_3_stocks.dropna(subset=['log_return'])
portfolio1_3_stocks = portfolio1_3_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns1_3 = portfolio1_3_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns1_3 = log_returns1_3 .mean().values
expected_returns1_3  = expected_returns1_3.reshape(-1, 1)
cov_matrix1_3  = log_returns1_3.cov()

n_assets1_3  = expected_returns1_3.shape[0]

company_names1_3  = log_returns1_3.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights1_3  = cp.Variable(n_assets1_3 )
portfolio_return1_3  = expected_returns1_3 .T @ weights1_3
portfolio_risk1_3  = cp.quad_form(weights1_3 , cov_matrix1_3 )

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns1_3 = []
efficient_frontier_risks1_3 = []
sharpe_ratios1_3 = []
gamma_values_with_sharpe1_3 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem1_3 = cp.Problem(cp.Maximize(portfolio_return1_3 - gamma * portfolio_risk1_3),
                         [cp.sum(weights1_3) == 1, weights1_3 >= 0])
    problem1_3.solve()
    annualized_return1_3 = np.exp(portfolio_return1_3.value * 252) - 1
    annualized_volatility1_3 = np.sqrt(portfolio_risk1_3.value) * np.sqrt(252)
    efficient_frontier_returns1_3.append(annualized_return1_3.item())
    efficient_frontier_risks1_3.append(annualized_volatility1_3.item())

    sharpe_ratio1_3 = (annualized_return1_3 - risk_free_rate) / annualized_volatility1_3
    sharpe_ratios1_3.append(sharpe_ratio1_3)
    gamma_values_with_sharpe1_3.append(gamma.value)

# Target volatility
target_volatility = 0.20
efficient_frontier_risks1_3 = np.array(efficient_frontier_risks1_3)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks1_3 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns1_3[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights1_3.value):
    if weight > 0:
        print(f"{company_names1_3[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks1_3, efficient_frontier_returns1_3, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks1_3[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

import cvxpy as cp
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import norm


start_date3 = '2022-11-30'
end_date3 = '2023-11-08'
official_data_period3 = official_data[(official_data['MarketDate'] >= start_date3) &
                                      (official_data['MarketDate'] <= end_date3)]

# Identify the core node
core_node2_3 = community_representatives3_max[0]


# Find the 1st cousins of the core node
first_cousins2_3 = [node for node in G_semi_period3.neighbors(core_node2_3)]

filtered_cousins2_3 = [node for node in first_cousins2_3 if abs(corr_matrix_period3_semi[core_node2_3][node]) > 0.7]

portfolio2_3_stocks = official_data_period3[official_data_period3['DsCmpyName'].isin(filtered_cousins2_3)]

# Calculate log returns
portfolio2_3_stocks['log_return'] = np.log(portfolio2_3_stocks['adjclose'] / portfolio2_3_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio2_3_stocks = portfolio2_3_stocks.dropna(subset=['log_return'])
portfolio2_3_stocks = portfolio2_3_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns2_3 = portfolio2_3_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns2_3 = log_returns2_3.mean().values
expected_returns2_3 = expected_returns2_3.reshape(-1, 1)
cov_matrix2_3 = log_returns2_3.cov()

n_assets2_3 = expected_returns2_3.shape[0]

company_names2_3 = log_returns2_3.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights2_3 = cp.Variable(n_assets2_3)
portfolio_return2_3 = expected_returns2_3.T @ weights2_3
portfolio_risk2_3 = cp.quad_form(weights2_3, cov_matrix2_3)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns2_3 = []
efficient_frontier_risks2_3 = []
sharpe_ratios2_3 = []
gamma_values_with_sharpe2_3 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem2_3 = cp.Problem(cp.Maximize(portfolio_return2_3 - gamma * portfolio_risk2_3),
                         [cp.sum(weights2_3) == 1, weights2_3 >= 0])
    problem2_3.solve()
    annualized_return2_3 = np.exp(portfolio_return2_3.value * 252) - 1
    annualized_volatility2_3 = np.sqrt(portfolio_risk2_3.value) * np.sqrt(252)
    efficient_frontier_returns2_3.append(annualized_return2_3.item())
    efficient_frontier_risks2_3.append(annualized_volatility2_3.item())

    sharpe_ratio2_3 = (annualized_return2_3 - risk_free_rate) / annualized_volatility2_3
    sharpe_ratios2_3.append(sharpe_ratio2_3)
    gamma_values_with_sharpe2_3.append(gamma.value)


# Target volatility
target_volatility = 0.20
efficient_frontier_risks2_3 = np.array(efficient_frontier_risks2_3)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks2_3 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns2_3[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights2_3.value):
    if weight > 0:
        print(f"{company_names2_3[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks2_3, efficient_frontier_returns2_3, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks2_3[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

import cvxpy as cp
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import norm

start_date3 = '2022-11-30'
end_date3 = '2023-11-08'
official_data_period3 = official_data[(official_data['MarketDate'] >= start_date3) &
                                      (official_data['MarketDate'] <= end_date3)]


# Identify the core node
core_node3_3 = community_representatives3_max[1]

# Find the 1st cousins of the core node
first_cousins3_3 = [node for node in G_semi_period3.neighbors(core_node3_3)]

filtered_cousins3_3 = [node for node in first_cousins3_3 if abs(corr_matrix_period3_semi[core_node3_3][node]) > 0.7]

portfolio3_3_stocks = official_data_period3[official_data_period3['DsCmpyName'].isin(filtered_cousins3_3)]

# Calculate log returns
portfolio3_3_stocks['log_return'] = np.log(portfolio3_3_stocks['adjclose'] / portfolio3_3_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio3_3_stocks = portfolio3_3_stocks.dropna(subset=['log_return'])
portfolio3_3_stocks = portfolio3_3_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns3_3 = portfolio3_3_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns3_3 = log_returns3_3.mean().values
expected_returns3_3 = expected_returns3_3.reshape(-1, 1)
cov_matrix3_3 = log_returns3_3.cov()

n_assets3_3 = expected_returns3_3.shape[0]

company_names3_3 = log_returns3_3.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights3_3 = cp.Variable(n_assets3_3)
portfolio_return3_3 = expected_returns3_3.T @ weights3_3
portfolio_risk3_3 = cp.quad_form(weights3_3, cov_matrix3_3)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns3_3 = []
efficient_frontier_risks3_3 = []
sharpe_ratios3_3 = []
gamma_values_with_sharpe3_3 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem3_3 = cp.Problem(cp.Maximize(portfolio_return3_3 - gamma * portfolio_risk3_3),
                         [cp.sum(weights3_3) == 1, weights3_3 >= 0])
    problem3_3.solve()
    annualized_return3_3 = np.exp(portfolio_return3_3.value * 252) - 1
    annualized_volatility3_3 = np.sqrt(portfolio_risk3_3.value) * np.sqrt(252)
    efficient_frontier_returns3_3.append(annualized_return3_3.item())
    efficient_frontier_risks3_3.append(annualized_volatility3_3.item())

    sharpe_ratio3_3 = (annualized_return3_3 - risk_free_rate) / annualized_volatility3_3
    sharpe_ratios3_3.append(sharpe_ratio3_3)
    gamma_values_with_sharpe3_3.append(gamma.value)


# Target volatility
target_volatility = 0.20
efficient_frontier_risks3_3 = np.array(efficient_frontier_risks3_3)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks3_3 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns3_3[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights3_3.value):
    if weight > 0:
        print(f"{company_names3_3[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks3_3, efficient_frontier_returns3_3, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks3_3[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

import cvxpy as cp
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import norm



start_date3 = '2022-11-30'
end_date3 = '2023-11-08'
official_data_period3 = official_data[(official_data['MarketDate'] >= start_date3) &
                                      (official_data['MarketDate'] <= end_date3)]

# Identify the core node
core_node4_3 = community_representatives3_max[2]


# Find the 1st cousins of the core node
first_cousins4_3 = [node for node in G_semi_period3.neighbors(core_node4_3)]

filtered_cousins4_3 = [node for node in first_cousins4_3 if abs(corr_matrix_period3_semi[core_node4_3][node]) > 0.7]

portfolio4_3_stocks = official_data_period3[official_data_period3['DsCmpyName'].isin(filtered_cousins4_3)]

# Calculate log returns
portfolio4_3_stocks['log_return'] = np.log(portfolio4_3_stocks['adjclose'] / portfolio4_3_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio4_3stocks = portfolio4_3_stocks.dropna(subset=['log_return'])
portfolio4_3_stocks = portfolio4_3_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns4_3 = portfolio4_3_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns4_3 = log_returns4_3.mean().values
expected_returns4_3 = expected_returns4_3.reshape(-1, 1)
cov_matrix4_3 = log_returns4_3.cov()

n_assets4_3 = expected_returns4_3.shape[0]

company_names4_3 = log_returns4_3.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights4_3 = cp.Variable(n_assets4_3)
portfolio_return4_3 = expected_returns4_3.T @ weights4_3
portfolio_risk4_3 = cp.quad_form(weights4_3, cov_matrix4_3)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns4_3 = []
efficient_frontier_risks4_3 = []
sharpe_ratios4_3 = []
gamma_values_with_sharpe4_3 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem4_3 = cp.Problem(cp.Maximize(portfolio_return4_3 - gamma * portfolio_risk4_3),
                         [cp.sum(weights4_3) == 1, weights4_3 >= 0])
    problem4_3.solve()
    annualized_return4_3 = np.exp(portfolio_return4_3.value * 252) - 1
    annualized_volatility4_3 = np.sqrt(portfolio_risk4_3.value) * np.sqrt(252)
    efficient_frontier_returns4_3.append(annualized_return4_3.item())
    efficient_frontier_risks4_3.append(annualized_volatility4_3.item())

    sharpe_ratio4_3 = (annualized_return4_3 - risk_free_rate) / annualized_volatility4_3
    sharpe_ratios4_3.append(sharpe_ratio4_3)
    gamma_values_with_sharpe4_3.append(gamma.value)


# Target volatility
target_volatility = 0.20
efficient_frontier_risks4_3 = np.array(efficient_frontier_risks4_3)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks4_3 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns4_3[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights4_3.value):
    if weight > 0:
        print(f"{company_names4_3[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks4_3, efficient_frontier_returns4_3, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks4_3[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

node_to_community3_0 = partition3
portfolio_stocks3_0 = list(G_semi_period3.nodes)
portfolio_stocks_community3_0 = [stock for stock in portfolio_stocks3_0 if stock in G_semi_period3.nodes() and node_to_community3_0.get(stock, -1) == 0]

start_date3 = '2022-11-30'
end_date3 = '2023-11-08'
official_data_period3 = official_data[(official_data['MarketDate'] >= start_date3) &
                                      (official_data['MarketDate'] <= end_date3)]

portfolio5_3_stocks = official_data_period3[official_data_period3['DsCmpyName'].isin(portfolio_stocks_community3_0)]

# Calculate log returns
portfolio5_3_stocks['log_return'] = np.log(portfolio5_3_stocks['adjclose'] / portfolio5_3_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio5_3_stocks = portfolio5_3_stocks.dropna(subset=['log_return'])
portfolio5_3_stocks = portfolio5_3_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns5_3 = portfolio5_3_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns5_3 = log_returns5_3.mean().values
expected_returns5_3 = expected_returns5_3.reshape(-1, 1)
cov_matrix5_3 = log_returns5_3.cov()

n_assets5_3 = expected_returns5_3.shape[0]

company_names5_3 = log_returns5_3.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights5_3 = cp.Variable(n_assets5_3)
portfolio_return5_3 = expected_returns5_3.T @ weights5_3
portfolio_risk5_3 = cp.quad_form(weights5_3, cov_matrix5_3)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns5_3 = []
efficient_frontier_risks5_3 = []
sharpe_ratios5_3 = []
gamma_values_with_sharpe5_3 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem5_3 = cp.Problem(cp.Maximize(portfolio_return5_3 - gamma * portfolio_risk5_3),
                         [cp.sum(weights5_3) == 1, weights5_3 >= 0])
    problem5_3.solve()
    annualized_return5_3 = np.exp(portfolio_return5_3.value * 252) - 1
    annualized_volatility5_3 = np.sqrt(portfolio_risk5_3.value) * np.sqrt(252)
    efficient_frontier_returns5_3.append(annualized_return5_3.item())
    efficient_frontier_risks5_3.append(annualized_volatility5_3.item())

    sharpe_ratio5_3 = (annualized_return5_3 - risk_free_rate) / annualized_volatility5_3
    sharpe_ratios5_3.append(sharpe_ratio5_3)
    gamma_values_with_sharpe5_3.append(gamma.value)


# Target volatility
target_volatility = 0.20
efficient_frontier_risks5_3 = np.array(efficient_frontier_risks5_3)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks5_3 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns5_3[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights5_3.value):
    if weight > 0:
        print(f"{company_names5_3[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks5_3, efficient_frontier_returns5_3, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks5_3[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

node_to_community3_1 = partition3
portfolio_stocks3_1 = list(G_semi_period3.nodes)
portfolio_stocks_community3_1= [stock for stock in portfolio_stocks3_1 if stock in G_semi_period3.nodes() and node_to_community3_1.get(stock, -1) == 1]

start_date3 = '2022-11-30'
end_date3 = '2023-11-08'
official_data_period3 = official_data[(official_data['MarketDate'] >= start_date3) &
                                      (official_data['MarketDate'] <= end_date3)]

portfolio6_3_stocks = official_data_period3[official_data_period3['DsCmpyName'].isin(portfolio_stocks_community3_1)]

# Calculate log returns
portfolio6_3_stocks['log_return'] = np.log(portfolio6_3_stocks['adjclose'] / portfolio6_3_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio6_3_stocks = portfolio6_3_stocks.dropna(subset=['log_return'])
portfolio6_3_stocks = portfolio6_3_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns6_3 = portfolio6_3_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns6_3 = log_returns6_3.mean().values
expected_returns6_3 = expected_returns6_3.reshape(-1, 1)
cov_matrix6_3 = log_returns6_3.cov()

n_assets6_3 = expected_returns6_3.shape[0]

company_names6_3 = log_returns6_3.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights6_3 = cp.Variable(n_assets6_3)
portfolio_return6_3 = expected_returns6_3.T @ weights6_3
portfolio_risk6_3 = cp.quad_form(weights6_3, cov_matrix6_3)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns6_3 = []
efficient_frontier_risks6_3 = []
sharpe_ratios6_3 = []
gamma_values_with_sharpe6_3 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem6_3 = cp.Problem(cp.Maximize(portfolio_return6_3 - gamma * portfolio_risk6_3),
                         [cp.sum(weights6_3) == 1, weights6_3 >= 0])
    problem6_3.solve()
    annualized_return6_3 = np.exp(portfolio_return6_3.value * 252) - 1
    annualized_volatility6_3 = np.sqrt(portfolio_risk6_3.value) * np.sqrt(252)
    efficient_frontier_returns6_3.append(annualized_return6_3.item())
    efficient_frontier_risks6_3.append(annualized_volatility6_3.item())

    sharpe_ratio6_3 = (annualized_return6_3 - risk_free_rate) / annualized_volatility6_3
    sharpe_ratios6_3.append(sharpe_ratio6_3)
    gamma_values_with_sharpe6_3.append(gamma.value)


# Target volatility
target_volatility = 0.20
efficient_frontier_risks6_3 = np.array(efficient_frontier_risks6_3)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks6_3 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns6_3[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights6_3.value):
    if weight > 0:
        print(f"{company_names6_3[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks6_3, efficient_frontier_returns6_3, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks6_3[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

node_to_community3_2 = partition3
portfolio_stocks3_2 = list(G_semi_period3.nodes)
portfolio_stocks_community3_2 = [stock for stock in portfolio_stocks3_2 if stock in G_semi_period3.nodes() and node_to_community3_2.get(stock, -1) == 2]

start_date3 = '2022-11-30'
end_date3 = '2023-11-08'
official_data_period3 = official_data[(official_data['MarketDate'] >= start_date3) &
                                      (official_data['MarketDate'] <= end_date3)]

portfolio7_3_stocks = official_data_period3[official_data_period3['DsCmpyName'].isin(portfolio_stocks_community3_2)]

# Calculate log returns
portfolio7_3_stocks['log_return'] = np.log(portfolio7_3_stocks['adjclose'] / portfolio7_3_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio7_3_stocks = portfolio7_3_stocks.dropna(subset=['log_return'])
portfolio7_3_stocks = portfolio7_3_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns7_3 = portfolio7_3_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns7_3 = log_returns7_3.mean().values
expected_returns7_3 = expected_returns7_3.reshape(-1, 1)
cov_matrix7_3 = log_returns7_3.cov()

n_assets7_3 = expected_returns7_3.shape[0]

company_names7_3 = log_returns7_3.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights7_3 = cp.Variable(n_assets7_3)
portfolio_return7_3 = expected_returns7_3.T @ weights7_3
portfolio_risk7_3 = cp.quad_form(weights7_3, cov_matrix7_3)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns7_3 = []
efficient_frontier_risks7_3 = []
sharpe_ratios7_3 = []
gamma_values_with_sharpe7_3 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem7_3 = cp.Problem(cp.Maximize(portfolio_return7_3 - gamma * portfolio_risk7_3),
                         [cp.sum(weights7_3) == 1, weights7_3 >= 0])
    problem7_3.solve()
    annualized_return7_3 = np.exp(portfolio_return7_3.value * 252) - 1
    annualized_volatility7_3 = np.sqrt(portfolio_risk7_3.value) * np.sqrt(252)
    efficient_frontier_returns7_3.append(annualized_return7_3.item())
    efficient_frontier_risks7_3.append(annualized_volatility7_3.item())

    sharpe_ratio7_3 = (annualized_return7_3 - risk_free_rate) / annualized_volatility7_3
    sharpe_ratios7_3.append(sharpe_ratio7_3)
    gamma_values_with_sharpe7_3.append(gamma.value)


# Target volatility
target_volatility = 0.20
efficient_frontier_risks7_3 = np.array(efficient_frontier_risks7_3)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks7_3 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns7_3[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights7_3.value):
    if weight > 0:
        print(f"{company_names7_3[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks7_3, efficient_frontier_returns7_3, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks7_3[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

representative_stocks3_1 = []
for community_id, stock in community_representatives3_max_extra.items():
    representative_stocks3_1.append(stock)

flat_representative_stocks3_1 = [stock for tuple in representative_stocks3_1 for stock in tuple]

start_date3 = '2022-11-30'
end_date3 = '2023-11-08'
official_data_period3 = official_data[(official_data['MarketDate'] >= start_date3) &
                                      (official_data['MarketDate'] <= end_date3)]

portfolio8_3_stocks = official_data_period3[official_data_period3['DsCmpyName'].isin(flat_representative_stocks3_1)]

# Calculate log returns
portfolio8_3_stocks['log_return'] = np.log(portfolio8_3_stocks['adjclose'] / portfolio8_3_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio8_3_stocks = portfolio8_3_stocks.dropna(subset=['log_return'])
portfolio8_3_stocks = portfolio8_3_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns8_3 = portfolio8_3_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns8_3 = log_returns8_3.mean().values
expected_returns8_3 = expected_returns8_3.reshape(-1, 1)
cov_matrix8_3 = log_returns8_3.cov()

n_assets8_3 = expected_returns8_3.shape[0]

company_names8_3 = log_returns8_3.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights8_3 = cp.Variable(n_assets8_3)
portfolio_return8_3 = expected_returns8_3.T @ weights8_3
portfolio_risk8_3 = cp.quad_form(weights8_3, cov_matrix8_3)


# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns8_3 = []
efficient_frontier_risks8_3 = []
sharpe_ratios8_3 = []
gamma_values_with_sharpe8_3 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem8_3 = cp.Problem(cp.Maximize(portfolio_return8_3 - gamma * portfolio_risk8_3),
                         [cp.sum(weights8_3) == 1, weights8_3 >= 0])
    problem8_3.solve()
    annualized_return8_3 = np.exp(portfolio_return8_3.value * 252) - 1
    annualized_volatility8_3 = np.sqrt(portfolio_risk8_3.value) * np.sqrt(252)
    efficient_frontier_returns8_3.append(annualized_return8_3.item())
    efficient_frontier_risks8_3.append(annualized_volatility8_3.item())

    sharpe_ratio8_3 = (annualized_return8_3 - risk_free_rate) / annualized_volatility8_3
    sharpe_ratios8_3.append(sharpe_ratio8_3)
    gamma_values_with_sharpe8_3.append(gamma.value)


# Target volatility
target_volatility = 0.20
efficient_frontier_risks8_3 = np.array(efficient_frontier_risks8_3)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks8_3 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns8_3[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights8_3.value):
    if weight > 0:
        print(f"{company_names8_3[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks8_3, efficient_frontier_returns8_3, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks8_3[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

representative_stocks3_2 = []
for community_id, stock in community_representatives3_max.items():
    representative_stocks3_2.append(stock)
    same_community_stocks3_2 = [node for node, comm in partition3.items() if comm == community_id]
    same_community_stocks3_2.remove(stock)
    if same_community_stocks3_2:
        representative_stocks3_2.append(same_community_stocks3_2[0])

start_date3 = '2022-11-30'
end_date3 = '2023-11-08'
official_data_period3 = official_data[(official_data['MarketDate'] >= start_date3) &
                                      (official_data['MarketDate'] <= end_date3)]

portfolio9_3_stocks = official_data_period3[official_data_period3['DsCmpyName'].isin(representative_stocks3_2)]

# Calculate log returns
portfolio9_3_stocks['log_return'] = np.log(portfolio9_3_stocks['adjclose'] / portfolio9_3_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio9_3_stocks = portfolio9_3_stocks.dropna(subset=['log_return'])
portfolio9_3_stocks = portfolio9_3_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns9_3 = portfolio9_3_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns9_3  = log_returns9_3.mean().values
expected_returns9_3 = expected_returns9_3.reshape(-1, 1)
cov_matrix9_3 = log_returns9_3.cov()

n_assets9_3  = expected_returns9_3.shape[0]

company_names9_3 = log_returns9_3.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights9_3 = cp.Variable(n_assets9_3)
portfolio_return9_3 = expected_returns9_3.T @ weights9_3
portfolio_risk9_3 = cp.quad_form(weights9_3, cov_matrix9_3)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns9_3 = []
efficient_frontier_risks9_3 = []
sharpe_ratios9_3 = []
gamma_values_with_sharpe9_3 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem9_3 = cp.Problem(cp.Maximize(portfolio_return9_3 - gamma * portfolio_risk9_3),
                         [cp.sum(weights9_3) == 1, weights9_3 >= 0])
    problem9_3.solve()
    annualized_return9_3 = np.exp(portfolio_return9_3.value * 252) - 1
    annualized_volatility9_3 = np.sqrt(portfolio_risk9_3.value) * np.sqrt(252)
    efficient_frontier_returns9_3.append(annualized_return9_3.item())
    efficient_frontier_risks9_3.append(annualized_volatility9_3.item())

    sharpe_ratio9_3 = (annualized_return9_3 - risk_free_rate) / annualized_volatility9_3
    sharpe_ratios9_3.append(sharpe_ratio9_3)
    gamma_values_with_sharpe9_3.append(gamma.value)


# Target volatility
target_volatility = 0.20
efficient_frontier_risks9_3 = np.array(efficient_frontier_risks9_3)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks9_3 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns9_3[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights9_3.value):
    if weight > 0:
        print(f"{company_names9_3[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks9_3, efficient_frontier_returns9_3, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks9_3[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

critical_nodes_all3 = set(critical_nodes_bc3 + critical_nodes_degree3)
for u, v, _ in strongest_edges3:
    critical_nodes_all3.add(u)
    critical_nodes_all3.add(v)

representative_stocks_mst3 = [node_to_company3[node] for node in critical_nodes_all3]

start_date3 = '2022-11-30'
end_date3 = '2023-11-08'
official_data_period3 = official_data[(official_data['MarketDate'] >= start_date3) &
                                      (official_data['MarketDate'] <= end_date3)]

portfolio10_3_stocks = official_data_period3[official_data_period3['DsCmpyName'].isin(representative_stocks_mst3)]

# Calculate log returns
portfolio10_3_stocks['log_return'] = np.log(portfolio10_3_stocks['adjclose'] / portfolio10_3_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio10_3_stocks = portfolio10_3_stocks.dropna(subset=['log_return'])
portfolio10_3_stocks = portfolio10_3_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns10_3 = portfolio10_3_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns10_3  = log_returns10_3.mean().values
expected_returns10_3 = expected_returns10_3.reshape(-1, 1)
cov_matrix10_3 = log_returns10_3.cov()

n_assets10_3  = expected_returns10_3.shape[0]

company_names10_3 = log_returns10_3.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights10_3 = cp.Variable(n_assets10_3)
portfolio_return10_3 = expected_returns10_3.T @ weights10_3
portfolio_risk10_3 = cp.quad_form(weights10_3, cov_matrix10_3)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns10_3 = []
efficient_frontier_risks10_3 = []
sharpe_ratios10_3 = []
gamma_values_with_sharpe10_3 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem10_3 = cp.Problem(cp.Maximize(portfolio_return10_3 - gamma * portfolio_risk10_3),
                         [cp.sum(weights10_3) == 1, weights10_3 >= 0])
    problem10_3.solve()
    annualized_return10_3 = np.exp(portfolio_return10_3.value * 252) - 1
    annualized_volatility10_3 = np.sqrt(portfolio_risk10_3.value) * np.sqrt(252)
    efficient_frontier_returns10_3.append(annualized_return10_3.item())
    efficient_frontier_risks10_3.append(annualized_volatility10_3.item())

    sharpe_ratio10_3 = (annualized_return10_3 - risk_free_rate) / annualized_volatility10_3
    sharpe_ratios10_3.append(sharpe_ratio10_3)
    gamma_values_with_sharpe10_3.append(gamma.value)


# Target volatility
target_volatility = 0.20
efficient_frontier_risks10_3 = np.array(efficient_frontier_risks10_3)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks10_3 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns10_3[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights10_3.value):
    if weight > 0:
        print(f"{company_names10_3[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks10_3, efficient_frontier_returns10_3, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks10_3[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()