# -*- coding: utf-8 -*-
"""Portfolios with Lower Volatility - Period 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K4hOhU6iufxwEtSxpzO133rKxJjPHhcC
"""

from google.colab import drive
drive.mount('/content/drive')
!ls "/content/drive/My Drive/thesis_files/"

import community
import networkx as nx
from community import community_louvain
import matplotlib.cm as cm
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cvxpy as cp

import pandas as pd
#clean and sort semiconductor companies
file_path =  "/content/drive/My Drive/thesis_files/Semiconductor Prices 2000 - 2023.csv"
data = pd.read_csv(file_path)
data = data.dropna(subset=['adjclose'])
data = data.sort_values(by=["InfoCode", "MarketDate"])
desired_companies = ['NVIDIA CORP.','TAIWAN SEMICON.MNFG.CTD.', 'RAMBUS INCO.', 'BROADCOM PTE LTD.', 'SAMSUNG SDI COMPANY LTD.','QUALCOMM INCO.','INTEL CORP.', 'TEXAS INSTRUMENTS INCO.', 'MICRON TECHNOLOGY INCO.', 'ARM HOLDINGS PLC.', 'ANALOG DEVICES INCO.', 'SK HYNIX INCO.', 'KLA CORP.', 'NXP SEMICONDUCTORS NV', 'MEDIATEK INCO.' , 'MICROCHIP TECH.INCO.','INFINEON TECHNOLOGIES AG', 'STMICROELECTRONICS NV',  'MONOLITHIC PWR.SYS.INCO.', 'ON SEMICONDUCTOR CORP.', 'LASERTEC CORP.','ENTEGRIS INCO.', 'SKYWORKS SOLUTIONS INCO.', 'BE SEMICONDUCTOR INDS.',  'QORVO INCO.', 'LATTICE SEMICON.CORP.', 'REALTEK SEMICON.CORP.',   'GLOBALWAFERS CTD.',  'COHERENT INCO.', 'AMKOR TECH.INCO.', 'ALCHIP TECHNOLOGIES LTD.', 'MACOM TGSN.HDG.INCO.',  'NANYA TECHNOLOGY CORP.', 'ROHM COMPANY LIMITED', 'EMEMORY TECHNOLOGY INCO.', 'GLOBAL UNICHIP CORP.' , 'CIRRUS LOGIC INCO.', 'SILICON LABS.INCO.', 'WOLFSPEED INCO.', 'AXCELIS TECHS.INCO.'  ]

filtered_data = data[data['DsCmpyName'].isin(desired_companies)]

start_date = '2016-01-01'
end_date = '2023-11-08'
official_data = filtered_data[(filtered_data['MarketDate'] >= start_date) & (filtered_data['MarketDate'] <= end_date)]

# Calculate log returns
official_data['log_return'] = official_data.groupby('InfoCode')['adjclose'].transform(lambda x: np.log(x / x.shift(1)))
# Fill missing log returns with rolling mean of the last 2 values
official_data['log_return'] = official_data.groupby('InfoCode')['log_return'].transform(lambda x: x.fillna(x.rolling(2, min_periods=1).mean()))
official_data = official_data[official_data['InfoCode'] != 2231]
official_data

##pivot the semiconductor data and organize better
semi_sort_more = official_data.sort_values(by='MarketDate')
semi_sort_more = official_data.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data
pivot_data = semi_sort_more.pivot(index='MarketDate', columns='DsCmpyName', values='adjclose')
pivot_data.reset_index(inplace=True)
pivot_data['MarketDate'] = pd.to_datetime(pivot_data['MarketDate'])

def fill_with_previous_average(df):
    for column in df.select_dtypes(include=[np.number]):
        for i in range(2, len(df)):
            if np.isnan(df.at[df.index[i], column]):
                non_nan_values = df[column][:i].dropna().tail(2)
                if len(non_nan_values) == 2:
                    df.at[df.index[i], column] = non_nan_values.mean()
    return df

pivot_data_numeric = pivot_data.drop(columns=['MarketDate'])


pivot_data_numeric_filled = fill_with_previous_average(pivot_data_numeric)


pivot_data_filled = pd.concat([pivot_data[['MarketDate']], pivot_data_numeric_filled], axis=1)

pivot_data_filled

##period 1 of semiconductor data (network) + calculate structural entropy
period1_start = '2016-01-01'
period1_end = '2018-12-31'
period1_data_semi = pivot_data_filled[(pivot_data_filled['MarketDate'] >= period1_start) & (pivot_data_filled['MarketDate'] <= period1_end)]
period1_data_semi.reset_index(drop=True, inplace=True)
corr_matrix_period1_semi = period1_data_semi.corr(method='pearson')

G_semi_period1 = nx.Graph()

thresh1 = 0.7
for i in corr_matrix_period1_semi.columns:
  for j in corr_matrix_period1_semi.columns:
    if i != j and abs(corr_matrix_period1_semi[i][j]) > thresh1:
        G_semi_period1.add_edge(i, j)

# # # # Apply Louvain community detection
partition1 = community_louvain.best_partition(G_semi_period1)
modularity1 = community_louvain.modularity(partition1, G_semi_period1)

values1 = [partition1.get(node) for node in G_semi_period1.nodes()]

node_to_company = {}


for node in G_semi_period1.nodes():
    if node in official_data['DsCmpyName'].values:
        company_name = official_data.loc[official_data['DsCmpyName'] == node, 'DsCmpyName'].iloc[0]
        node_to_company[node] = company_name


plt.figure(figsize=(10, 10))
pos = nx.spring_layout(G_semi_period1)
nx.draw(G_semi_period1, pos, cmap=plt.get_cmap('jet'), node_color=values1, node_size=30, with_labels=False)
nx.draw_networkx_labels(G_semi_period1, pos, labels={node: name for node, name in node_to_company.items() if node in G_semi_period1.nodes()}, font_size=8)  # Add labels
plt.show()



print("Total number of communities:", len(set(values1)))


# Calculate the sizes of each community
community_sizes1 = np.array([list(values1).count(i) for i in set(values1)])

# Compute the probability distribution of community sizes
prob_distribution1 = community_sizes1 / np.sum(community_sizes1)

# Calculate the entropy
structural_entropy1 = -np.sum(prob_distribution1 * np.log(prob_distribution1))

print(structural_entropy1)

import numpy as np
import networkx as nx
import pandas as pd
import matplotlib.pyplot as plt
import cvxpy as cp


company_columns = [col for col in corr_matrix_period1_semi.columns if col != 'MarketDate']


corr_matrix_period1_semi_filtered = corr_matrix_period1_semi.loc[company_columns, company_columns]

# Transform correlations to distances
corr_matrix_array1 = corr_matrix_period1_semi_filtered.to_numpy()
corr_matrix_array1_transformed = 1 - corr_matrix_array1

G1_mst = nx.from_numpy_array(corr_matrix_array1_transformed)

# Find the minimum spanning tree
mst1 = nx.minimum_spanning_tree(G1_mst)


strongest_edges = sorted(mst1.edges(data=True), key=lambda x: x[2]['weight'])[:2]
print("Edges with strongest correlations based on edge weights:", strongest_edges)



node_degrees1 = dict(mst1.degree())
critical_nodes_degree1 = [node for node, degree in node_degrees1.items() if degree == max(node_degrees1.values())]
print("Critical nodes based on node degrees:", critical_nodes_degree1)


betweenness_centrality1 = nx.betweenness_centrality(mst1)
critical_nodes_bc1 = [node for node, bc in betweenness_centrality1.items() if bc == max(betweenness_centrality1.values())]
print("Critical nodes based on betweenness centrality:", critical_nodes_bc1)

node_to_company1 = {i: company for i, company in enumerate(corr_matrix_period1_semi_filtered.columns)}

plt.figure(figsize=(10, 8))
pos1 = nx.spring_layout(mst1)
nx.draw(mst1, pos1, labels=node_to_company1, with_labels=True, node_size=300, node_color='skyblue', edge_color='k', linewidths=0.5)
nx.draw_networkx_nodes(mst1, pos1, nodelist=critical_nodes_degree1, node_color='red', node_size=500)
nx.draw_networkx_nodes(mst1, pos1, nodelist=critical_nodes_bc1, node_color='green', node_size=500)
nx.draw_networkx_edges(mst1, pos1, edgelist=[(u, v) for u, v, _ in strongest_edges], edge_color='blue', width=2)
plt.title('Minimum Spanning Tree for Semiconductor Companies - Period 1')
plt.show()

##FIND HIGHEST CENTRALITY - PERIOD 1
from networkx.algorithms.centrality import eigenvector_centrality
# Define weights for centrality measures
weights = {'eigenvector_centrality': 1/3, 'closeness_centrality': 1/3, 'betweenness_centrality': 1/3}

# Find representative stocks for each community based on centrality measures
community_representatives1_max = {}
for community_id in set(values1):
    community_nodes = [node for node, comm in partition1.items() if comm == community_id]
    representative_stock1_max = max(community_nodes, key=lambda x:
                               weights['eigenvector_centrality'] * nx.eigenvector_centrality(G_semi_period1)[x] +
                               weights['closeness_centrality'] * nx.closeness_centrality(G_semi_period1)[x] +
                               weights['betweenness_centrality'] * nx.betweenness_centrality(G_semi_period1)[x])
    community_representatives1_max[community_id] = representative_stock1_max

print("Representative stocks for each community for Period 1 (Max):")
for community_id, stock in community_representatives1_max.items():
    print(f"Community {community_id}: {stock}")


weights = {'eigenvector_centrality': 1/3, 'closeness_centrality': 1/3, 'betweenness_centrality': 1/3}

community_representatives1_min = {}
for community_id in set(values1):
    community_nodes = [node for node, comm in partition1.items() if comm == community_id]
    representative_stock1_min = min(community_nodes, key=lambda x:
                               weights['eigenvector_centrality'] * nx.eigenvector_centrality(G_semi_period1)[x] +
                               weights['closeness_centrality'] * nx.closeness_centrality(G_semi_period1)[x] +
                               weights['betweenness_centrality'] * nx.betweenness_centrality(G_semi_period1)[x])
    community_representatives1_min[community_id] = representative_stock1_min

print("Representative stocks for each community for Period 1 (Min):")
for community_id, stock in community_representatives1_min.items():
    print(f"Community {community_id}: {stock}")

#FIND HIGHEST AND 2ND HIGHEST CENTRALITY - PERIOD 1
weights = {'eigenvector_centrality': 1/3, 'closeness_centrality': 1/3, 'betweenness_centrality': 1/3}

# Find representative stocks for each community based on centrality measures
community_representatives1_max_extra = {}
community_representatives1_min_extra = {}

for community_id in set(values1):
    community_nodes = [node for node, comm in partition1.items() if comm == community_id]

    sorted_nodes = sorted(community_nodes, key=lambda x: (
        weights['eigenvector_centrality'] * nx.eigenvector_centrality(G_semi_period1)[x] +
        weights['closeness_centrality'] * nx.closeness_centrality(G_semi_period1)[x] +
        weights['betweenness_centrality'] * nx.betweenness_centrality(G_semi_period1)[x]
    ), reverse=True)  # Sort in descending order for highest centrality

    # Select top 2 stocks with highest centrality
    top_2_max = sorted_nodes[:2]
    community_representatives1_max_extra[community_id] = top_2_max

    sorted_nodes_reverse = sorted(community_nodes, key=lambda x: (
        weights['eigenvector_centrality'] * nx.eigenvector_centrality(G_semi_period1)[x] +
        weights['closeness_centrality'] * nx.closeness_centrality(G_semi_period1)[x] +
        weights['betweenness_centrality'] * nx.betweenness_centrality(G_semi_period1)[x]
    ))

    # Select top 2 stocks with lowest centrality
    top_2_min = sorted_nodes_reverse[:2]
    community_representatives1_min_extra[community_id] = top_2_min

print("Top 2 representative stocks for each community for Period 1 (Max):")
for community_id, stocks in community_representatives1_max_extra.items():
    print(f"Community {community_id}: {stocks}")

print("\nTop 2 representative stocks for each community for Period 1 (Min):")
for community_id, stocks in community_representatives1_min_extra.items():
    print(f"Community {community_id}: {stocks}")

import cvxpy as cp
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import norm
from scipy import interpolate


start_date = '2016-01-01'
end_date = '2018-12-31'
official_data_period = official_data[(official_data['MarketDate'] >= start_date) &
                                     (official_data['MarketDate'] <= end_date)]

max_centrality_stocks = list(community_representatives1_max.values())

portfolio_stocks = official_data_period[official_data_period['DsCmpyName'].isin(max_centrality_stocks)]

# Calculate log returns
portfolio_stocks['log_return'] = np.log(portfolio_stocks['adjclose'] / portfolio_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio_stocks.loc[:, 'log_return'] = np.log(portfolio_stocks['adjclose'] / portfolio_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio_stocks = portfolio_stocks.dropna(subset=['log_return'])
portfolio_stocks = portfolio_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns = portfolio_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns = log_returns.mean().values
expected_returns = expected_returns.reshape(-1, 1)
cov_matrix = log_returns.cov()

n_assets = expected_returns.shape[0]


company_names = log_returns.columns.tolist()

# Risk-free rate
risk_free_rate = 0.02

weights = cp.Variable(n_assets)
portfolio_return = expected_returns.T @ weights
portfolio_risk = cp.quad_form(weights, cov_matrix)

efficient_frontier_returns = []
efficient_frontier_risks = []

gamma_values = np.logspace(-6, 3, 50)
for gamma_value in gamma_values:
    gamma = cp.Parameter(nonneg=True, value=gamma_value)
    objective = cp.Maximize(portfolio_return - gamma * portfolio_risk)
    constraints = [cp.sum(weights) == 1, weights >= 0]
    problem = cp.Problem(objective, constraints)
    problem.solve()


    annualized_return = np.exp(portfolio_return.value * 252) - 1
    annualized_volatility = np.sqrt(portfolio_risk.value * 252)
    efficient_frontier_returns.append(annualized_return)
    efficient_frontier_risks.append(annualized_volatility)


efficient_frontier_returns = np.array(efficient_frontier_returns)
efficient_frontier_risks = np.array(efficient_frontier_risks)

# Target volatility
target_volatility = 0.20

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks - target_volatility))


expected_return_at_20_percent_volatility = efficient_frontier_returns[index_closest_volatility][0]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights.value):
    if weight > 0:
        print(f"{company_names[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks, efficient_frontier_returns, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import cvxpy as cp
from scipy import interpolate
import pandas as pd
from scipy.stats import norm


# 1st NEIGHBORS PORTFOLIO FOR COMMUNITY 1
start_date = '2016-01-01'
end_date = '2018-12-31'
official_data_period1 = official_data[(official_data['MarketDate'] >= start_date) &
                                      (official_data['MarketDate'] <= end_date)]


core_node = community_representatives1_max[0]


# Find the 1st cousins of the core node
first_cousins = [node for node in G_semi_period1.neighbors(core_node)]

filtered_cousins = [node for node in first_cousins if abs(corr_matrix_period1_semi[core_node][node]) > 0.7]

portfolio2_stocks = official_data_period1[official_data_period1['DsCmpyName'].isin(filtered_cousins)]

# Calculate log returns
portfolio2_stocks['log_return'] = np.log(portfolio2_stocks['adjclose'] / portfolio2_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio2_stocks = portfolio2_stocks.dropna(subset=['log_return'])
portfolio2_stocks = portfolio2_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns2 = portfolio2_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns2 = log_returns2.mean().values
expected_returns2 = expected_returns2.reshape(-1, 1)
cov_matrix2 = log_returns2.cov()

n_assets2 = expected_returns2.shape[0]


company_names2 = log_returns2.columns.tolist()


# Calculate expected returns and covariance matrix
expected_returns2 = log_returns2.mean().values
cov_matrix2 = log_returns2.cov().values

n_assets2 = expected_returns2.shape[0]

weights2 = cp.Variable(n_assets2)
portfolio_return2 = expected_returns2.T @ weights2
portfolio_risk2 = cp.quad_form(weights2, cov_matrix2)

efficient_frontier_returns2 = []
efficient_frontier_risks2 = []

gamma_values = np.logspace(-6, 3, 50)
for gamma_value in gamma_values:
    gamma = cp.Parameter(nonneg=True, value=gamma_value)
    objective = cp.Maximize(portfolio_return2 - gamma * portfolio_risk2)
    constraints = [cp.sum(weights2) == 1, weights2 >= 0]
    problem = cp.Problem(objective, constraints)
    problem.solve()

    annualized_return2 = np.exp(portfolio_return2.value * 252) - 1
    annualized_volatility2 = np.sqrt(portfolio_risk2.value * 252)
    efficient_frontier_returns2.append(annualized_return2)
    efficient_frontier_risks2.append(annualized_volatility2)

efficient_frontier_returns2 = np.array(efficient_frontier_returns2)
efficient_frontier_risks2 = np.array(efficient_frontier_risks2)

# Target volatility
target_volatility = 0.20

f_interp = interpolate.interp1d(efficient_frontier_risks2, efficient_frontier_returns2, kind='linear')
target_return = f_interp(target_volatility)

index_target_volatility = np.argmax(efficient_frontier_risks2 <= target_volatility)

# Print the annualized expected return on the efficient frontier with volatility of 20% or less
annualized_return_at_target_volatility = efficient_frontier_returns2[index_target_volatility]
print(f"Annualized Expected Return on the Efficient Frontier with Volatility <= 20%: {annualized_return_at_target_volatility:.4%}")

# Print the weights for each company that are above 0
print("\nCompany Names and Weights for the Targeted Portfolio:")
for i, weight in enumerate(weights2.value):
    if weight > 0:
        print(f"{company_names2[i]}: {weight:.2%}")

# Plot the efficient frontier with the targeted portfolio
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks2, efficient_frontier_returns2, 'b-o', label='Efficient Frontier')
plt.scatter(target_volatility, target_return, color='red', label='Targeted Portfolio (<= 20% Volatility)')
plt.title('Efficient Frontier and Targeted Portfolio')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

import cvxpy as cp
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import norm

# 1st NEIGHBORS PORTFOLIO FOR COMMUNITY 2
start_date = '2016-01-01'
end_date = '2018-12-31'
official_data_period1 = official_data[(official_data['MarketDate'] >= start_date) &
                                      (official_data['MarketDate'] <= end_date)]


core_node2 = community_representatives1_max[1]

# Find the 1st cousins of the core node
first_cousins2 = [node for node in G_semi_period1.neighbors(core_node2)]

filtered_cousins2 = [node for node in first_cousins2 if abs(corr_matrix_period1_semi[core_node2][node]) > 0.7]

portfolio3_stocks = official_data_period1[official_data_period1['DsCmpyName'].isin(filtered_cousins2)]

# Calculate log returns
portfolio3_stocks['log_return'] = np.log(portfolio3_stocks['adjclose'] / portfolio3_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio3_stocks = portfolio3_stocks.dropna(subset=['log_return'])
portfolio3_stocks = portfolio3_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns3 = portfolio3_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns3 = log_returns3.mean().values
expected_returns3 = expected_returns3.reshape(-1, 1)
cov_matrix3 = log_returns3.cov()

n_assets3 = expected_returns3.shape[0]

company_names3 = log_returns3.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights3 = cp.Variable(n_assets3)
portfolio_return3 = expected_returns3.T @ weights3
portfolio_risk3 = cp.quad_form(weights3, cov_matrix3)

efficient_frontier_returns3 = []
efficient_frontier_risks3 = []

gamma_values = np.logspace(-6, 3, 50)
for gamma_value in gamma_values:
    gamma = cp.Parameter(nonneg=True, value=gamma_value)
    objective = cp.Maximize(portfolio_return3 - gamma * portfolio_risk3)
    constraints = [cp.sum(weights3) == 1, weights3 >= 0]
    problem = cp.Problem(objective, constraints)
    problem.solve()

    annualized_return3 = np.exp(portfolio_return3.value * 252) - 1
    annualized_volatility3 = np.sqrt(portfolio_risk3.value * 252)
    efficient_frontier_returns3.append(annualized_return3)
    efficient_frontier_risks3.append(annualized_volatility3)

efficient_frontier_returns3 = np.array(efficient_frontier_returns3)
efficient_frontier_risks3 = np.array(efficient_frontier_risks3)

# Target volatility
target_volatility = 0.20

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks3 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns3[index_closest_volatility][0]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights3.value):
    if weight > 0:
        print(f"{company_names3[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks3, efficient_frontier_returns3, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks3[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

node_to_community1_0 = partition1
portfolio_stocks1_0 = list(G_semi_period1.nodes)
portfolio_stocks_community1_0 = [stock for stock in portfolio_stocks1_0 if stock in G_semi_period1.nodes() and node_to_community1_0.get(stock, -1) == 0]

start_date = '2016-01-01'
end_date = '2018-12-31'
official_data_period1 = official_data[(official_data['MarketDate'] >= start_date) &
                                      (official_data['MarketDate'] <= end_date)]

portfolio5_stocks = official_data_period1[official_data_period1['DsCmpyName'].isin(portfolio_stocks_community1_0)]

# Calculate log returns
portfolio5_stocks['log_return'] = np.log(portfolio5_stocks['adjclose'] / portfolio5_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio5_stocks = portfolio5_stocks.dropna(subset=['log_return'])
portfolio5_stocks = portfolio5_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns5 = portfolio5_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns5 = log_returns5.mean().values
expected_returns5 = expected_returns5.reshape(-1, 1)
cov_matrix5 = log_returns5.cov()

n_assets5 = expected_returns5.shape[0]

company_names5 = log_returns5.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights5 = cp.Variable(n_assets5)
portfolio_return5 = expected_returns5.T @ weights5
portfolio_risk5 = cp.quad_form(weights5, cov_matrix5)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns5 = []
efficient_frontier_risks5 = []
sharpe_ratios5= []
gamma_values_with_sharpe5 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem5 = cp.Problem(cp.Maximize(portfolio_return5 - gamma * portfolio_risk5),
                         [cp.sum(weights5) == 1, weights5 >= 0])
    problem5.solve()
    annualized_return5 = np.exp(portfolio_return5.value * 252) - 1
    annualized_volatility5 = np.sqrt(portfolio_risk5.value) * np.sqrt(252)
    efficient_frontier_returns5.append(annualized_return5.item())
    efficient_frontier_risks5.append(annualized_volatility5.item())

    sharpe_ratio5 = (annualized_return5 - risk_free_rate) / annualized_volatility5
    sharpe_ratios5.append(sharpe_ratio5)
    gamma_values_with_sharpe5.append(gamma.value)


# Target volatility
target_volatility = 0.20
efficient_frontier_risks5 = np.array(efficient_frontier_risks5)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks5 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns5[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights5.value):
    if weight > 0:
        print(f"{company_names5[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks5, efficient_frontier_returns5, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks5[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

node_to_community1_1 = partition1
portfolio_stocks1_1 = list(G_semi_period1.nodes)
portfolio_stocks_community1_1 = [stock for stock in portfolio_stocks1_1 if stock in G_semi_period1.nodes() and node_to_community1_1.get(stock, -1) == 1]

start_date = '2016-01-01'
end_date = '2018-12-31'
official_data_period1 = official_data[(official_data['MarketDate'] >= start_date) &
                                      (official_data['MarketDate'] <= end_date)]

portfolio6_stocks = official_data_period1[official_data_period1['DsCmpyName'].isin(portfolio_stocks_community1_1)]

# Calculate log returns
portfolio6_stocks['log_return'] = np.log(portfolio6_stocks['adjclose'] / portfolio6_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio6_stocks = portfolio6_stocks.dropna(subset=['log_return'])
portfolio6_stocks = portfolio6_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns6 = portfolio6_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns6 = log_returns6.mean().values
expected_returns6 = expected_returns6.reshape(-1, 1)
cov_matrix6 = log_returns6.cov()

n_assets6 = expected_returns6.shape[0]

company_names6 = log_returns6.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights6 = cp.Variable(n_assets6)
portfolio_return6 = expected_returns6.T @ weights6
portfolio_risk6 = cp.quad_form(weights6, cov_matrix6)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns6 = []
efficient_frontier_risks6 = []
sharpe_ratios6 = []
gamma_values_with_sharpe6 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem6 = cp.Problem(cp.Maximize(portfolio_return6 - gamma * portfolio_risk6),
                         [cp.sum(weights6) == 1, weights6 >= 0])
    problem6.solve()
    annualized_return6 = np.exp(portfolio_return6.value * 252) - 1
    annualized_volatility6 = np.sqrt(portfolio_risk6.value) * np.sqrt(252)
    efficient_frontier_returns6.append(annualized_return6.item())
    efficient_frontier_risks6.append(annualized_volatility6.item())

    sharpe_ratio6 = (annualized_return6 - risk_free_rate) / annualized_volatility6
    sharpe_ratios6.append(sharpe_ratio6)
    gamma_values_with_sharpe6.append(gamma.value)

# Target volatility
target_volatility = 0.20
efficient_frontier_risks6 = np.array(efficient_frontier_risks6)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks6 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns6[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights6.value):
    if weight > 0:
        print(f"{company_names6[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks6, efficient_frontier_returns6, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks6[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

representative_stocks1 = []
for community_id, stock in community_representatives1_max_extra.items():
    representative_stocks1.append(stock)

flat_representative_stocks1 = [stock for tuple in representative_stocks1 for stock in tuple]

start_date = '2016-01-01'
end_date = '2018-12-31'
official_data_period1 = official_data[(official_data['MarketDate'] >= start_date) &
                                      (official_data['MarketDate'] <= end_date)]

portfolio8_stocks = official_data_period1[official_data_period1['DsCmpyName'].isin(flat_representative_stocks1)]

# Calculate log returns
portfolio8_stocks['log_return'] = np.log(portfolio8_stocks['adjclose'] / portfolio8_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio8_stocks = portfolio8_stocks.dropna(subset=['log_return'])
portfolio8_stocks = portfolio8_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns8 = portfolio8_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns8 = log_returns8.mean().values
expected_returns8= expected_returns8.reshape(-1, 1)
cov_matrix8 = log_returns8.cov()

n_assets8 = expected_returns8.shape[0]

company_names8 = log_returns8.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights8 = cp.Variable(n_assets8)
portfolio_return8 = expected_returns8.T @ weights8
portfolio_risk8 = cp.quad_form(weights8, cov_matrix8)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns8 = []
efficient_frontier_risks8 = []
sharpe_ratios8 = []
gamma_values_with_sharpe8 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem8 = cp.Problem(cp.Maximize(portfolio_return8 - gamma * portfolio_risk8),
                         [cp.sum(weights8) == 1, weights8 >= 0])
    problem8.solve()
    annualized_return8 = np.exp(portfolio_return8.value * 252) - 1
    annualized_volatility8 = np.sqrt(portfolio_risk8.value) * np.sqrt(252)
    efficient_frontier_returns8.append(annualized_return8.item())
    efficient_frontier_risks8.append(annualized_volatility8.item())

    sharpe_ratio8 = (annualized_return8 - risk_free_rate) / annualized_volatility8
    sharpe_ratios8.append(sharpe_ratio8)
    gamma_values_with_sharpe8.append(gamma.value)

# Target volatility
target_volatility = 0.20
efficient_frontier_risks8 = np.array(efficient_frontier_risks8)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks8 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns8[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights8.value):
    if weight > 0:
        print(f"{company_names8[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks8, efficient_frontier_returns8, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks8[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

representative_stocks1_2 = []
for community_id, stock in community_representatives1_max.items():
    representative_stocks1_2.append(stock)
    same_community_stocks = [node for node, comm in partition1.items() if comm == community_id]
    same_community_stocks.remove(stock)
    if same_community_stocks:
        representative_stocks1_2.append(same_community_stocks[0])

start_date = '2016-01-01'
end_date = '2018-12-31'
official_data_period1 = official_data[(official_data['MarketDate'] >= start_date) &
                                      (official_data['MarketDate'] <= end_date)]

portfolio9_stocks = official_data_period1[official_data_period1['DsCmpyName'].isin(representative_stocks1_2)]

# Calculate log returns
portfolio9_stocks['log_return'] = np.log(portfolio9_stocks['adjclose'] / portfolio9_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio9_stocks = portfolio9_stocks.dropna(subset=['log_return'])
portfolio9_stocks = portfolio9_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns9 = portfolio9_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns9 = log_returns9.mean().values
expected_returns9 = expected_returns9.reshape(-1, 1)
cov_matrix9 = log_returns9.cov()

n_assets9 = expected_returns9.shape[0]

company_names9 = log_returns9.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights9 = cp.Variable(n_assets9)
portfolio_return9 = expected_returns9.T @ weights9
portfolio_risk9 = cp.quad_form(weights9, cov_matrix9)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns9 = []
efficient_frontier_risks9 = []
sharpe_ratios9 = []
gamma_values_with_sharpe9 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem9 = cp.Problem(cp.Maximize(portfolio_return9 - gamma * portfolio_risk9),
                         [cp.sum(weights9) == 1, weights9 >= 0])
    problem9.solve()
    annualized_return9 = np.exp(portfolio_return9.value * 252) - 1
    annualized_volatility9 = np.sqrt(portfolio_risk9.value) * np.sqrt(252)
    efficient_frontier_returns9.append(annualized_return9.item())
    efficient_frontier_risks9.append(annualized_volatility9.item())

    sharpe_ratio9 = (annualized_return9 - risk_free_rate) / annualized_volatility9
    sharpe_ratios9.append(sharpe_ratio9)
    gamma_values_with_sharpe9.append(gamma.value)

# Target volatility
target_volatility = 0.20
efficient_frontier_risks9 = np.array(efficient_frontier_risks9)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks9 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns9[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights9.value):
    if weight > 0:
        print(f"{company_names5[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks9, efficient_frontier_returns9, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks9[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()

critical_nodes_all1 = set(critical_nodes_bc1 + critical_nodes_degree1)
for u, v, _ in strongest_edges:
    critical_nodes_all1.add(u)
    critical_nodes_all1.add(v)


node_to_company1 = {i: company for i, company in enumerate(corr_matrix_period1_semi_filtered.columns)}

for node in mst1.nodes():
    if node not in node_to_company1:
        node_to_company1[node] = f"Unknown Company {node}"


representative_stocks_mst1 = [node_to_company1[node] for node in critical_nodes_all1]

# Find the stocks with the highest centrality measures from the MST
representative_stocks_mst1 = [node_to_company1[node] for node in critical_nodes_all1]

start_date = '2016-01-01'
end_date = '2018-12-31'
official_data_period1 = official_data[(official_data['MarketDate'] >= start_date) &
                                      (official_data['MarketDate'] <= end_date)]

portfolio10_stocks = official_data_period1[official_data_period1['DsCmpyName'].isin(representative_stocks_mst1)]

# Calculate log returns
portfolio10_stocks['log_return'] = np.log(portfolio10_stocks['adjclose'] / portfolio10_stocks.groupby('DsCmpyName')['adjclose'].shift(1))
portfolio10_stocks = portfolio10_stocks.dropna(subset=['log_return'])
portfolio10_stocks = portfolio10_stocks.drop_duplicates(subset=['MarketDate', 'DsCmpyName'])

# Pivot the data to get log returns
log_returns10 = portfolio10_stocks.pivot(index='MarketDate', columns='DsCmpyName', values='log_return').fillna(0)

# Calculate expected returns and covariance matrix
expected_returns10 = log_returns10.mean().values
expected_returns10 = expected_returns10.reshape(-1, 1)
cov_matrix10 = log_returns10.cov()

n_assets10 = expected_returns10.shape[0]

company_names10 = log_returns10.columns.tolist()

gamma = cp.Parameter(nonneg=True)
gamma_values = np.logspace(-4, 3, 50)

weights10 = cp.Variable(n_assets10)
portfolio_return10 = expected_returns10.T @ weights10
portfolio_risk10 = cp.quad_form(weights10, cov_matrix10)

# Risk-free rate
risk_free_rate = 0.02

efficient_frontier_returns10 = []
efficient_frontier_risks10 = []
sharpe_ratios10 = []
gamma_values_with_sharpe10 = []

for gamma_value in gamma_values:
    gamma.value = gamma_value
    problem10 = cp.Problem(cp.Maximize(portfolio_return10 - gamma * portfolio_risk10),
                         [cp.sum(weights10) == 1, weights10 >= 0])
    problem10.solve()
    annualized_return10 = np.exp(portfolio_return10.value * 252) - 1
    annualized_volatility10 = np.sqrt(portfolio_risk10.value) * np.sqrt(252)
    efficient_frontier_returns10.append(annualized_return10.item())
    efficient_frontier_risks10.append(annualized_volatility10.item())

    sharpe_ratio10 = (annualized_return10 - risk_free_rate) / annualized_volatility10
    sharpe_ratios10.append(sharpe_ratio10)
    gamma_values_with_sharpe10.append(gamma.value)

# Target volatility
target_volatility = 0.20
efficient_frontier_risks10 = np.array(efficient_frontier_risks10)

index_closest_volatility = np.argmin(np.abs(efficient_frontier_risks10 - target_volatility))

expected_return_at_20_percent_volatility = efficient_frontier_returns10[index_closest_volatility]

# Print the annualized expected return on the efficient frontier with volatility closest to 20%
print(f"Expected Return at 20% Volatility: {expected_return_at_20_percent_volatility:.4%}")

# Print the weights for each company that are above 0 in the portfolio at the closest point to 20% volatility
print("\nCompany Names and Weights for the Portfolio at 20% Volatility:")
for i, weight in enumerate(weights10.value):
    if weight > 0:
        print(f"{company_names10[i]}: {weight:.2%}")


# Plot the efficient frontier
plt.figure(figsize=(10, 6))
plt.plot(efficient_frontier_risks10, efficient_frontier_returns10, 'b-', label='Efficient Frontier')
plt.scatter(efficient_frontier_risks10[index_closest_volatility], expected_return_at_20_percent_volatility, color='red', label='Closest Point to 20% Volatility')
plt.title('Efficient Frontier and Closest Point to 20% Volatility')
plt.xlabel('Volatility (Standard Deviation)')
plt.ylabel('Expected Return')
plt.legend()
plt.grid(True)
plt.show()